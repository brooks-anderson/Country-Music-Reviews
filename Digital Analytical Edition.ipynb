{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c204d",
   "metadata": {},
   "source": [
    "# Country & Soul\n",
    "### *Analyzing Trends in American Music Journalism from 1960 to Present*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19f2c5",
   "metadata": {},
   "source": [
    "# Digital Analytical Edition\n",
    "#### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries & Set Up](#paragraph1)\n",
    "3. [Creating the LIB table](#paragraph2)\n",
    "    1. [Parsing Dates](#subparagraph1)\n",
    "    2. [Narrowing the Scope](#subparagraph2)\n",
    "4. [Constructing the Corpus](#paragraph3)\n",
    "5. [Extracting a Vocabulary](#paragraph4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330874",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>\n",
    "In this notebook, I aggregate the source files to create a Digital Analytical Edition (DAE).  \n",
    "\n",
    "The DAE contains the following tables:  \n",
    "+ `LIB.csv`    - Metadata for each article.\n",
    "+ `CORPUS.csv` - Aggregated text from source files. Indexed by Ordered Hierarchy of Content Object (OHCO) structure.\n",
    "+ `VOCAB.csv`  - Linguistic features and statistics for words in `CORPUS.csv`. Stop words removed.\n",
    "+ `BOW.csv`    - Bag-of-words representation of `CORPUS.csv` with stop words removed.\n",
    "\n",
    "Source files were scraped from [rocksbackpages.com](https://www.rocksbackpages.com/) using `rbpscraper.py`. I present an example of `rbpscraper.py` functionality here. It is **not** recommended that the reader attempts to run this code. `rbpscraper.py` requires a subscription to rocksbackpages.com and several hours of your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3feb3be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste URL:\n",
      "https://www.rocksbackpages.com/sample-search-url\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Please paste cookies:\n",
      "sample=cookies; AcceptedCookieNotice=true\n",
      "\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from rbpscraper import RBPScraper\n",
    "\n",
    "country = RBPScraper(desc = \"country\", write_path = \"./datadir/\")\n",
    "    # desc - description of articles to be scraped\n",
    "    # write_path - path to directory where articles will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ef282",
   "metadata": {},
   "source": [
    "After instantiating an RBPScraper object, the user will be prompted to enter the url for the first page of search results. Then, the user will be prompted to enter cookies for authentification. The web-scraping process can be initiated by calling the following methods. This will save articles as html files and a metadata table as a csv file to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd079578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country.searchScraper().articleScraper().writeLIB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cbd2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries & Set Up<a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa0e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "dataPath = \"./data/\"\n",
    "articlePath = \"./data/html/\"\n",
    "\n",
    "# define OHCO structure\n",
    "OHCO = ['article_id', 'para_id', 'sent_id', 'token_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f61b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Library Table<a name=\"paragraph2\"></a>\n",
    "Two library tables were created during the webscraping process. I combine, process, and refine the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in metadata table for each genre\n",
    "country_metadata = pd.read_csv(f\"{dataPath}cLIB.csv\")\n",
    "country_metadata.set_index(\"id\", inplace = True)\n",
    "\n",
    "soul_metadata = pd.read_csv(f\"{dataPath}sLIB.csv\")\n",
    "soul_metadata.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e86e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.concat([country_metadata, soul_metadata])\n",
    "\n",
    "LIB.index.rename(OHCO[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b231",
   "metadata": {},
   "source": [
    "#### Parsing Dates<a name=\"subparagraph1\"></a>\n",
    "Inspection reveals that the publication dates are not in a standard format. I define a function that parses date strings. The following table describes how dates are assigned to ambiguous formats.\n",
    "\n",
    "|`date` format|output format|\n",
    "|------------|-------------|\n",
    "|Month *year*|YYYY-mm-01|\n",
    "|*year*|YYYY-07-01|\n",
    "|Summer *year*|YYYY-08-01|\n",
    "|Fall *year*|YYYY-11-01|\n",
    "|Winter *year*|YYYY-02-01|\n",
    "|Spring *year*|YYYY-05-01|\n",
    "\n",
    "Any date string that is missing or does not adhere to one of the above formats will be corrected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_date(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%d %B %Y\")\n",
    "\n",
    "def year_only(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dt_obj = datetime.strptime(dt, \"%Y\").date()\n",
    "    return dt_obj.replace(month = 7, day = 1)\n",
    "\n",
    "def month_year(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%B %Y\").date()\n",
    "\n",
    "def season_year(dt):\n",
    "    from datetime import datetime, date\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        season = dt.split()[-2].lower()\n",
    "        year = dt.split()[-1]\n",
    "\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "    \n",
    "    if season == 'spring':\n",
    "        return date(int(year), 5, 1)\n",
    "    elif season == 'summer':\n",
    "        return date(int(year), 8, 1)\n",
    "    elif season == 'fall':\n",
    "        return date(int(year), 11, 1)\n",
    "    elif season == 'winter':\n",
    "        return date(int(year), 2, 1)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(dt):\n",
    "    '''\n",
    "    Converts publication date strings from rocksbackpages.com articles\n",
    "    to date objects with format YYYY-mm-dd.\n",
    "    '''\n",
    "    helpers = [normal_date, year_only, month_year, season_year]\n",
    "    \n",
    "    for f in helpers:\n",
    "        try:\n",
    "            return f(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['date_parsed'] = LIB.date.apply(date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4a1692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c610</th>\n",
       "      <td>Charley Pride, Tammy Wynette and George Jones:...</td>\n",
       "      <td>Gene Guerrero</td>\n",
       "      <td>The Great Speckled Bird</td>\n",
       "      <td>31 February 1972</td>\n",
       "      <td>['george-jones', 'tammy-wynette', 'charley-pri...</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=charley-pri...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12801</th>\n",
       "      <td>Duffy: Rockferry</td>\n",
       "      <td>Gavin Martin</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>29 February 2002</td>\n",
       "      <td>['duffy']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=duffy-irock...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12916</th>\n",
       "      <td>Earth Wind And Fire: Beacon Theater, New York</td>\n",
       "      <td>Kandia Crazy Horse</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>15 2003</td>\n",
       "      <td>['earth-wind--fire']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=earth-wind-...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "c610        Charley Pride, Tammy Wynette and George Jones:...   \n",
       "s12801                                       Duffy: Rockferry   \n",
       "s12916          Earth Wind And Fire: Beacon Theater, New York   \n",
       "\n",
       "                        author                   source              date  \\\n",
       "article_id                                                                  \n",
       "c610             Gene Guerrero  The Great Speckled Bird  31 February 1972   \n",
       "s12801            Gavin Martin             Daily Mirror  29 February 2002   \n",
       "s12916      Kandia Crazy Horse               PopMatters           15 2003   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "c610        ['george-jones', 'tammy-wynette', 'charley-pri...  country   \n",
       "s12801                                              ['duffy']     soul   \n",
       "s12916                                   ['earth-wind--fire']     soul   \n",
       "\n",
       "                   type                                               href  \\\n",
       "article_id                                                                   \n",
       "c610        Live Review  /Library/SearchLinkRedirect?folder=charley-pri...   \n",
       "s12801           Review  /Library/SearchLinkRedirect?folder=duffy-irock...   \n",
       "s12916      Live Review  /Library/SearchLinkRedirect?folder=earth-wind-...   \n",
       "\n",
       "           date_parsed  \n",
       "article_id              \n",
       "c610               NaT  \n",
       "s12801             NaT  \n",
       "s12916             NaT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.loc[LIB.date_parsed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0faab7",
   "metadata": {},
   "source": [
    "Only three dates failed to parse. I will manually correct them. After that, I will replace the `date` column with `date_parsed` as keeping both is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4121cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.loc['c610', 'date_parsed'] = date(1972, 2, 29)\n",
    "LIB.loc[\"s12801\", 'date_parsed'] = date(2002, 2, 28)\n",
    "LIB.loc[\"s12916\", 'date_parsed'] = date(2003, 7, 1)\n",
    "\n",
    "# replace date col\n",
    "LIB['date'] = LIB.date_parsed\n",
    "LIB.drop(columns ='date_parsed', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d185",
   "metadata": {},
   "source": [
    "Next, I notice that the `subjects` column is a string representation of a list--not an actual list. `subjects` is converted to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37dfe556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "LIB.subjects = LIB.subjects.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa665f",
   "metadata": {},
   "source": [
    "#### Narrowing the Scope<a name=\"subparagraph2\"></a>\n",
    "I refine the Focus of my analysis rather than work with all 4000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5861589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 articles missing a subject were dropped\n"
     ]
    }
   ],
   "source": [
    "# drop articles not tagged with a subject\n",
    "empty_subjects = [True if x == [] else False for x in LIB.subjects ]\n",
    "print(f\"{sum(empty_subjects)} articles missing a subject were dropped\")\n",
    "\n",
    "LIB = LIB[[not x for x in empty_subjects]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interview</th>\n",
       "      <th>Review</th>\n",
       "      <th>Live Review</th>\n",
       "      <th>Profile and Interview</th>\n",
       "      <th>Report and Interview</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Report</th>\n",
       "      <th>Book Excerpt</th>\n",
       "      <th>Obituary</th>\n",
       "      <th>Retrospective</th>\n",
       "      <th>...</th>\n",
       "      <th>Review and Interview</th>\n",
       "      <th>Film/DVD/TV Review</th>\n",
       "      <th>Guide</th>\n",
       "      <th>Audio transcript of interview</th>\n",
       "      <th>Discography</th>\n",
       "      <th>Special Feature</th>\n",
       "      <th>Readers' Letters</th>\n",
       "      <th>Column</th>\n",
       "      <th>Letters</th>\n",
       "      <th>Film/DVD Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>1016</td>\n",
       "      <td>905</td>\n",
       "      <td>762</td>\n",
       "      <td>301</td>\n",
       "      <td>155</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Interview  Review  Live Review  Profile and Interview  \\\n",
       "type       1016     905          762                    301   \n",
       "\n",
       "      Report and Interview  Profile  Report  Book Excerpt  Obituary  \\\n",
       "type                   155      109     106           104       101   \n",
       "\n",
       "      Retrospective  ...  Review and Interview  Film/DVD/TV Review  Guide  \\\n",
       "type             88  ...                    18                  16      6   \n",
       "\n",
       "      Audio transcript of interview  Discography  Special Feature  \\\n",
       "type                              5            5                4   \n",
       "\n",
       "      Readers' Letters  Column  Letters  Film/DVD Review  \n",
       "type                 3       1        1                1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refine article types\n",
    "\n",
    "# consolidate 'Sleeve and programme notes' with 'Sleevenotes'\n",
    "LIB.loc[LIB.type == 'Sleeve and programme notes', 'type'] = 'Sleevenotes'\n",
    "\n",
    "LIB.type.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd0ba5",
   "metadata": {},
   "source": [
    "Immediately, I see some document types that should be excluded: *Film/DVD Review*, *Letters*, *Readers' Letters*. These documents do not fit with the broader corpus as they are either not about music, or not written by professional journalists.  \n",
    "\n",
    "It is debatable whether *obituary* and *memoir* should be included. These document types are dissimilar from other documents in the corpus as they contain more biographical language. However, if an artist has an associated *obituary* or *memoir*, they were likely to have had significant cultural impact. This analysis is conducted at the level of genre rather than artist. Thus, I discard *obituary* and *memoir* in favor of reducing the size of the corpus.\n",
    "\n",
    "Lastly, I reason that *book excerpts* should also be removed. Out of the 104 book excerpts, 97 are sourced from a single book, *The Faber Companion to 20<sup>th</sup> Century Popular Music*. These excerpts suffer the same issues of unsuitability as the obituaries; they are more pragmatic than they are poetic. Additionally, including so many documents from a single source would almost certainly have an unintended effect on the analysis.\n",
    "\n",
    "The remaining documents discuss musicians, alblums, and performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec12b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 3651\n"
     ]
    }
   ],
   "source": [
    "exclude = ['Film/DVD Review', 'Film/DVD/TV Review',\n",
    "           'Letters', \"Readers' Letters\", \n",
    "           'Obituary', 'Memoir', \n",
    "           'Book Excerpt', 'Book Review',\n",
    "           'Audio transcript of interview']\n",
    "\n",
    "LIB = LIB.loc[~LIB.type.isin(exclude)]\n",
    "print(f\"Number of Articles: {len(LIB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79faf4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s5716</th>\n",
       "      <td>The Moments: The Moments Greatest Hits (Stang)</td>\n",
       "      <td>Joe McEwen</td>\n",
       "      <td>The Village Voice</td>\n",
       "      <td>1977-09-05 00:00:00</td>\n",
       "      <td>[moments-the]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=the-moments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11816</th>\n",
       "      <td>Smooth: Reality (A&amp;M)</td>\n",
       "      <td>Miles Marshall Lewis</td>\n",
       "      <td>L.A. Weekly</td>\n",
       "      <td>1998-03-26 00:00:00</td>\n",
       "      <td>[smooth]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=smooth-irea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3110</th>\n",
       "      <td>The O'Jays: Back Stabbers (Philadelphia Intern...</td>\n",
       "      <td>Danny Goldberg</td>\n",
       "      <td>Rolling Stone</td>\n",
       "      <td>1972-10-26 00:00:00</td>\n",
       "      <td>[harold-melvin--the-blue-notes, ojays]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=the-ojays-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s310</th>\n",
       "      <td>Sam Cooke: Pop singer shot dead</td>\n",
       "      <td>Ivor Davis</td>\n",
       "      <td>Daily Express</td>\n",
       "      <td>1964-12-12 00:00:00</td>\n",
       "      <td>[sam-cooke]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Report</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=sam-cooke-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s9018</th>\n",
       "      <td>Tina Turner: Mega Woman Conquers the World</td>\n",
       "      <td>Mark Rowland</td>\n",
       "      <td>Musician</td>\n",
       "      <td>1986-10-01 00:00:00</td>\n",
       "      <td>[tina-turner]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=tina-turner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11817</th>\n",
       "      <td>Donnie Fritts: Leanin' Man from Alabam'</td>\n",
       "      <td>Chris Bourke</td>\n",
       "      <td>Real Groove</td>\n",
       "      <td>1998-04-01 00:00:00</td>\n",
       "      <td>[donnie-fritts]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=donnie-frit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12317</th>\n",
       "      <td>Rae &amp; Christian</td>\n",
       "      <td>Bill Brewster</td>\n",
       "      <td>The Big Issue</td>\n",
       "      <td>2001-07-01 00:00:00</td>\n",
       "      <td>[rae--christian, bobby-womack]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=rae--christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2504</th>\n",
       "      <td>Shelby Lynne: Embassy Rooms, London ****</td>\n",
       "      <td>Lucy O'Brien</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>1999-10-27 00:00:00</td>\n",
       "      <td>[shelby-lynne]</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=shelby-lynn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12210</th>\n",
       "      <td>Aaliyah: The M Factor (Movies, Music, Motivation)</td>\n",
       "      <td>Jeff Lorez</td>\n",
       "      <td>Blues &amp; Soul</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "      <td>[aaliyah]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=aaliyah-the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s8709</th>\n",
       "      <td>Cameo: Hammersmith Odeon, London</td>\n",
       "      <td>Paul Sexton</td>\n",
       "      <td>Record Mirror</td>\n",
       "      <td>1985-12-21 00:00:00</td>\n",
       "      <td>[cameo]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=cameo-hamme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "s5716          The Moments: The Moments Greatest Hits (Stang)   \n",
       "s11816                                  Smooth: Reality (A&M)   \n",
       "s3110       The O'Jays: Back Stabbers (Philadelphia Intern...   \n",
       "s310                          Sam Cooke: Pop singer shot dead   \n",
       "s9018              Tina Turner: Mega Woman Conquers the World   \n",
       "s11817                Donnie Fritts: Leanin' Man from Alabam'   \n",
       "s12317                                        Rae & Christian   \n",
       "c2504                Shelby Lynne: Embassy Rooms, London ****   \n",
       "s12210      Aaliyah: The M Factor (Movies, Music, Motivation)   \n",
       "s8709                        Cameo: Hammersmith Odeon, London   \n",
       "\n",
       "                          author             source                 date  \\\n",
       "article_id                                                                 \n",
       "s5716                 Joe McEwen  The Village Voice  1977-09-05 00:00:00   \n",
       "s11816      Miles Marshall Lewis        L.A. Weekly  1998-03-26 00:00:00   \n",
       "s3110             Danny Goldberg      Rolling Stone  1972-10-26 00:00:00   \n",
       "s310                  Ivor Davis      Daily Express  1964-12-12 00:00:00   \n",
       "s9018               Mark Rowland           Musician  1986-10-01 00:00:00   \n",
       "s11817              Chris Bourke        Real Groove  1998-04-01 00:00:00   \n",
       "s12317             Bill Brewster      The Big Issue  2001-07-01 00:00:00   \n",
       "c2504               Lucy O'Brien       The Guardian  1999-10-27 00:00:00   \n",
       "s12210                Jeff Lorez       Blues & Soul  2000-07-01 00:00:00   \n",
       "s8709                Paul Sexton      Record Mirror  1985-12-21 00:00:00   \n",
       "\n",
       "                                          subjects    topic  \\\n",
       "article_id                                                    \n",
       "s5716                                [moments-the]     soul   \n",
       "s11816                                    [smooth]     soul   \n",
       "s3110       [harold-melvin--the-blue-notes, ojays]     soul   \n",
       "s310                                   [sam-cooke]     soul   \n",
       "s9018                                [tina-turner]     soul   \n",
       "s11817                             [donnie-fritts]     soul   \n",
       "s12317              [rae--christian, bobby-womack]     soul   \n",
       "c2504                               [shelby-lynne]  country   \n",
       "s12210                                   [aaliyah]     soul   \n",
       "s8709                                      [cameo]     soul   \n",
       "\n",
       "                             type  \\\n",
       "article_id                          \n",
       "s5716                      Review   \n",
       "s11816                     Review   \n",
       "s3110                      Review   \n",
       "s310                       Report   \n",
       "s9018                   Interview   \n",
       "s11817                  Interview   \n",
       "s12317      Profile and Interview   \n",
       "c2504                 Live Review   \n",
       "s12210                  Interview   \n",
       "s8709                 Live Review   \n",
       "\n",
       "                                                         href  \n",
       "article_id                                                     \n",
       "s5716       /Library/SearchLinkRedirect?folder=the-moments...  \n",
       "s11816      /Library/SearchLinkRedirect?folder=smooth-irea...  \n",
       "s3110       /Library/SearchLinkRedirect?folder=the-ojays-i...  \n",
       "s310        /Library/SearchLinkRedirect?folder=sam-cooke-p...  \n",
       "s9018       /Library/SearchLinkRedirect?folder=tina-turner...  \n",
       "s11817      /Library/SearchLinkRedirect?folder=donnie-frit...  \n",
       "s12317      /Library/SearchLinkRedirect?folder=rae--christian  \n",
       "c2504       /Library/SearchLinkRedirect?folder=shelby-lynn...  \n",
       "s12210      /Library/SearchLinkRedirect?folder=aaliyah-the...  \n",
       "s8709       /Library/SearchLinkRedirect?folder=cameo-hamme...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c5a62",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing the Corpus<a name=\"paragraph3\"></a>\n",
    "I read in the body of each document and tokenize it with `nltk`'s `sent_tokenize()` and `WhitespaceTokenizer()` methods. Tokens are amassed in the `CORPUS` table and indexed by document OHCO structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7908aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for reading files\n",
    "def RBP_reader(filePath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(filePath, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, 'html')\n",
    "    \n",
    "    writer = soup.find(\"span\", class_=\"writer\") \\\n",
    "                 .get_text() \\\n",
    "                 .replace(r'\\n+', ' ') \\\n",
    "                 .strip()\n",
    "    \n",
    "    standfirst = soup.find(\"div\", class_=\"standfirst\").get_text()\n",
    "    copy = soup.find(\"div\", class_=\"copy\").get_text()\n",
    "\n",
    "    # if standfirst contains writer name, it is assumed that \n",
    "    # standfirst is purely metadata and should be ignored\n",
    "    if writer.lower() in standfirst.lower():\n",
    "        doc = copy\n",
    "    else:\n",
    "        doc = standfirst+copy\n",
    "    \n",
    "    return doc.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(library, filePrefix = \"./data/html/\"):\n",
    "    '''\n",
    "    Inputs\n",
    "    -----\n",
    "    library - pandas dataframe, must include author_id as index\n",
    "    fileprefix - string, path to directory containing .html files\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    CORPUS - pandas dataframe, contains all tokens in corpus\n",
    "    '''\n",
    "    import re\n",
    "    import nltk\n",
    "    \n",
    "    para_pat = r\"\\n{2,}\"\n",
    "    documents = []\n",
    "    \n",
    "    i=0\n",
    "    numIter = len(library)\n",
    "    \n",
    "    for id in library.index:\n",
    "        \n",
    "        doc = RBP_reader(filePrefix+str(id)+\".html\")\n",
    "        \n",
    "        PARAS = re.split(para_pat, doc)\n",
    "        PARAS = [x.strip().replace('\\n', ' ') for x in PARAS]\n",
    "        PARAS = pd.DataFrame(PARAS, columns = ['para_str'])\n",
    "        PARAS.index.name = 'para_id'\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame('sent_str')\n",
    "        \n",
    "        SENTS.index.names = ['para_id', 'sent_id']\n",
    "        \n",
    "        SENTS = SENTS.sent_str.str.replace(\"-\", \" \").to_frame()\n",
    "        SENTS = SENTS.sent_str.str.replace(\"/\", \" \").to_frame()\n",
    "        \n",
    "        TOKENS = SENTS.sent_str\\\n",
    "                      .apply(lambda x: \\\n",
    "                             pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))) \\\n",
    "                      .stack() \\\n",
    "                      .to_frame('pos_tuple')\n",
    "\n",
    "        TOKENS.index.names = OHCO[1:]\n",
    "        \n",
    "        TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "        TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "        TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "        \n",
    "        punc_pos = ['$', \"''\", '(', ')', '[', ']', ',', '--', '.', ':', '``']\n",
    "        TOKENS['term_str'] = TOKENS[~TOKENS.pos.isin(punc_pos)].token_str \\\n",
    "                                .str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS['article_id'] = id\n",
    "        TOKENS = TOKENS.reset_index().set_index(OHCO)\n",
    "        \n",
    "        documents.append(TOKENS)\n",
    "        \n",
    "        if i % round(numIter/5) == 0:\n",
    "            print(f\"{round(i*100/numIter)}% Complete\")\n",
    "        i +=1\n",
    "\n",
    "    \n",
    "    CORPUS = pd.concat(documents).sort_index()\n",
    "    \n",
    "    del(documents)\n",
    "    del(PARAS)\n",
    "    del(SENTS)\n",
    "    del(TOKENS)\n",
    "    \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be5dfb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% Complete\n",
      "20% Complete\n",
      "40% Complete\n",
      "60% Complete\n",
      "80% Complete\n",
      "100% Complete\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac35542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank tokens and NANs\n",
    "CORPUS = CORPUS[CORPUS.term_str!='']\n",
    "CORPUS = CORPUS[~CORPUS.term_str.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "689194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS.to_csv('./data/CORPUS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f5612",
   "metadata": {},
   "source": [
    "**Adding article length to `LIB`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81a170a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['length'] = CORPUS.groupby('article_id').term_str.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833cd37",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting a Vocabulary<a name=\"paragraph4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b1fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VOCAB table\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba38c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max part-of-speech\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "\n",
    "# add number of POS associated with each term\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "996a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add term-based statistics\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
