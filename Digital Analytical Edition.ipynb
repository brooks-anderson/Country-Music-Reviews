{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c204d",
   "metadata": {},
   "source": [
    "# Country & Soul\n",
    "### *Analyzing Trends in American Music Journalism from 1960 to Present*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19f2c5",
   "metadata": {},
   "source": [
    "# Digital Analytical Edition\n",
    "#### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries & Set Up](#paragraph1)\n",
    "3. [Creating the LIB table](#paragraph2)\n",
    "    1. [Parsing Dates](#subparagraph1)\n",
    "    2. [Narrowing the Scope](#subparagraph2)\n",
    "4. [Constructing the Corpus](#paragraph3)\n",
    "5. [Extracting a Vocabulary](#paragraph4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330874",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>\n",
    "In this notebook, I aggregate the source files to create a Digital Analytical Edition (DAE).  \n",
    "\n",
    "The DAE contains the following tables:  \n",
    "+ `LIB.csv`    - Metadata for each article.\n",
    "+ `CORPUS.csv` - Aggregated text from source files. Indexed by Ordered Hierarchy of Content Object (OHCO) structure.\n",
    "+ `VOCAB.csv`  - Linguistic features and statistics for words in `CORPUS.csv`. Stop words removed.\n",
    "+ `BOW.csv`    - Bag-of-words representation of `CORPUS.csv` with stop words removed.\n",
    "\n",
    "Source files were scraped from [rocksbackpages.com](https://www.rocksbackpages.com/) using `rbpscraper.py`. I present an example of `rbpscraper.py` functionality here. It is **not** recommended that the reader attempts to run this code. `rbpscraper.py` requires a subscription to rocksbackpages.com and several hours of your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3feb3be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste URL:\n",
      "https://www.rocksbackpages.com/sample-search-url\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Please paste cookies:\n",
      "sample=cookies; AcceptedCookieNotice=true\n",
      "\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from rbpscraper import RBPScraper\n",
    "\n",
    "country = RBPScraper(desc = \"country\", write_path = \"./datadir/\")\n",
    "    # desc - description of articles to be scraped\n",
    "    # write_path - path to directory where articles will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ef282",
   "metadata": {},
   "source": [
    "After instantiating an RBPScraper object, the user will be prompted to enter the url for the first page of search results. Then, the user will be prompted to enter cookies for authentification. The web-scraping process can be initiated by calling the following methods. This will save articles as html files and a metadata table as a csv file to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd079578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country.searchScraper().articleScraper().writeLIB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cbd2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries & Set Up<a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa0e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "dataPath = \"./data/\"\n",
    "articlePath = \"./data/html/\"\n",
    "\n",
    "# define OHCO structure\n",
    "OHCO = ['article_id', 'para_id', 'sent_id', 'token_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f61b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Library Table<a name=\"paragraph2\"></a>\n",
    "Two library tables were created during the webscraping process. I combine, process, and refine the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in metadata table for each genre\n",
    "country_metadata = pd.read_csv(f\"{dataPath}cLIB.csv\")\n",
    "country_metadata.set_index(\"id\", inplace = True)\n",
    "\n",
    "soul_metadata = pd.read_csv(f\"{dataPath}sLIB.csv\")\n",
    "soul_metadata.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e86e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.concat([country_metadata, soul_metadata])\n",
    "\n",
    "LIB.index.rename(OHCO[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b231",
   "metadata": {},
   "source": [
    "#### Parsing Dates<a name=\"subparagraph1\"></a>\n",
    "Inspection reveals that the publication dates are not in a standard format. I define a function that parses date strings. The following table describes how dates are assigned to ambiguous formats.\n",
    "\n",
    "|`date` format|output format|\n",
    "|------------|-------------|\n",
    "|Month *year*|YYYY-mm-01|\n",
    "|*year*|YYYY-07-01|\n",
    "|Summer *year*|YYYY-08-01|\n",
    "|Fall *year*|YYYY-11-01|\n",
    "|Winter *year*|YYYY-02-01|\n",
    "|Spring *year*|YYYY-05-01|\n",
    "\n",
    "Any date string that is missing or does not adhere to one of the above formats will be corrected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_date(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%d %B %Y\")\n",
    "\n",
    "def year_only(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dt_obj = datetime.strptime(dt, \"%Y\").date()\n",
    "    return dt_obj.replace(month = 7, day = 1)\n",
    "\n",
    "def month_year(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%B %Y\").date()\n",
    "\n",
    "def season_year(dt):\n",
    "    from datetime import datetime, date\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        season = dt.split()[-2].lower()\n",
    "        year = dt.split()[-1]\n",
    "\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "    \n",
    "    if season == 'spring':\n",
    "        return date(int(year), 5, 1)\n",
    "    elif season == 'summer':\n",
    "        return date(int(year), 8, 1)\n",
    "    elif season == 'fall':\n",
    "        return date(int(year), 11, 1)\n",
    "    elif season == 'winter':\n",
    "        return date(int(year), 2, 1)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(dt):\n",
    "    '''\n",
    "    Converts publication date strings from rocksbackpages.com articles\n",
    "    to date objects with format YYYY-mm-dd.\n",
    "    '''\n",
    "    helpers = [normal_date, year_only, month_year, season_year]\n",
    "    \n",
    "    for f in helpers:\n",
    "        try:\n",
    "            return f(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['date_parsed'] = LIB.date.apply(date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4a1692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c610</th>\n",
       "      <td>Charley Pride, Tammy Wynette and George Jones:...</td>\n",
       "      <td>Gene Guerrero</td>\n",
       "      <td>The Great Speckled Bird</td>\n",
       "      <td>31 February 1972</td>\n",
       "      <td>['george-jones', 'tammy-wynette', 'charley-pri...</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=charley-pri...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12801</th>\n",
       "      <td>Duffy: Rockferry</td>\n",
       "      <td>Gavin Martin</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>29 February 2002</td>\n",
       "      <td>['duffy']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=duffy-irock...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12916</th>\n",
       "      <td>Earth Wind And Fire: Beacon Theater, New York</td>\n",
       "      <td>Kandia Crazy Horse</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>15 2003</td>\n",
       "      <td>['earth-wind--fire']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=earth-wind-...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "c610        Charley Pride, Tammy Wynette and George Jones:...   \n",
       "s12801                                       Duffy: Rockferry   \n",
       "s12916          Earth Wind And Fire: Beacon Theater, New York   \n",
       "\n",
       "                        author                   source              date  \\\n",
       "article_id                                                                  \n",
       "c610             Gene Guerrero  The Great Speckled Bird  31 February 1972   \n",
       "s12801            Gavin Martin             Daily Mirror  29 February 2002   \n",
       "s12916      Kandia Crazy Horse               PopMatters           15 2003   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "c610        ['george-jones', 'tammy-wynette', 'charley-pri...  country   \n",
       "s12801                                              ['duffy']     soul   \n",
       "s12916                                   ['earth-wind--fire']     soul   \n",
       "\n",
       "                   type                                               href  \\\n",
       "article_id                                                                   \n",
       "c610        Live Review  /Library/SearchLinkRedirect?folder=charley-pri...   \n",
       "s12801           Review  /Library/SearchLinkRedirect?folder=duffy-irock...   \n",
       "s12916      Live Review  /Library/SearchLinkRedirect?folder=earth-wind-...   \n",
       "\n",
       "           date_parsed  \n",
       "article_id              \n",
       "c610               NaT  \n",
       "s12801             NaT  \n",
       "s12916             NaT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.loc[LIB.date_parsed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0faab7",
   "metadata": {},
   "source": [
    "Only three dates failed to parse. I will manually correct them. After that, I will replace the `date` column with `date_parsed` as keeping both is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4121cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.loc['c610', 'date_parsed'] = date(1972, 2, 29)\n",
    "LIB.loc[\"s12801\", 'date_parsed'] = date(2002, 2, 28)\n",
    "LIB.loc[\"s12916\", 'date_parsed'] = date(2003, 7, 1)\n",
    "\n",
    "# replace date col\n",
    "LIB['date'] = LIB.date_parsed\n",
    "LIB.drop(columns ='date_parsed', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d185",
   "metadata": {},
   "source": [
    "Next, I notice that the `subjects` column is a string representation of a list--not an actual list. `subjects` is converted to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37dfe556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "LIB.subjects = LIB.subjects.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa665f",
   "metadata": {},
   "source": [
    "#### Narrowing the Scope<a name=\"subparagraph2\"></a>\n",
    "I refine the Focus of my analysis rather than work with all 4000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5861589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 articles missing a subject were dropped\n"
     ]
    }
   ],
   "source": [
    "# drop articles not tagged with a subject\n",
    "empty_subjects = [True if x == [] else False for x in LIB.subjects ]\n",
    "print(f\"{sum(empty_subjects)} articles missing a subject were dropped\")\n",
    "\n",
    "LIB = LIB[[not x for x in empty_subjects]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interview</th>\n",
       "      <th>Review</th>\n",
       "      <th>Live Review</th>\n",
       "      <th>Profile and Interview</th>\n",
       "      <th>Report and Interview</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Report</th>\n",
       "      <th>Book Excerpt</th>\n",
       "      <th>Obituary</th>\n",
       "      <th>Retrospective</th>\n",
       "      <th>...</th>\n",
       "      <th>Review and Interview</th>\n",
       "      <th>Film/DVD/TV Review</th>\n",
       "      <th>Guide</th>\n",
       "      <th>Audio transcript of interview</th>\n",
       "      <th>Discography</th>\n",
       "      <th>Special Feature</th>\n",
       "      <th>Readers' Letters</th>\n",
       "      <th>Column</th>\n",
       "      <th>Letters</th>\n",
       "      <th>Film/DVD Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>1016</td>\n",
       "      <td>905</td>\n",
       "      <td>762</td>\n",
       "      <td>301</td>\n",
       "      <td>155</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Interview  Review  Live Review  Profile and Interview  \\\n",
       "type       1016     905          762                    301   \n",
       "\n",
       "      Report and Interview  Profile  Report  Book Excerpt  Obituary  \\\n",
       "type                   155      109     106           104       101   \n",
       "\n",
       "      Retrospective  ...  Review and Interview  Film/DVD/TV Review  Guide  \\\n",
       "type             88  ...                    18                  16      6   \n",
       "\n",
       "      Audio transcript of interview  Discography  Special Feature  \\\n",
       "type                              5            5                4   \n",
       "\n",
       "      Readers' Letters  Column  Letters  Film/DVD Review  \n",
       "type                 3       1        1                1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refine article types\n",
    "\n",
    "# consolidate 'Sleeve and programme notes' with 'Sleevenotes'\n",
    "LIB.loc[LIB.type == 'Sleeve and programme notes', 'type'] = 'Sleevenotes'\n",
    "\n",
    "LIB.type.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd0ba5",
   "metadata": {},
   "source": [
    "Immediately, I see some document types that should be excluded: *Film/DVD Review*, *Letters*, *Readers' Letters*. These documents do not fit with the broader corpus as they are either not about music, or not written by professional journalists.  \n",
    "\n",
    "It is debatable whether *obituary* and *memoir* should be included. These document types are dissimilar from other documents in the corpus as they contain more biographical language. However, if an artist has an associated *obituary* or *memoir*, they were likely to have had significant cultural impact. This analysis is conducted at the level of genre rather than artist. Thus, I discard *obituary* and *memoir* in favor of reducing the size of the corpus.\n",
    "\n",
    "Lastly, I reason that *book excerpts* should also be removed. Out of the 104 book excerpts, 97 are sourced from a single book, *The Faber Companion to 20<sup>th</sup> Century Popular Music*. These excerpts suffer the same issues of unsuitability as the obituaries; they are more pragmatic than they are poetic. Additionally, including so many documents from a single source would almost certainly have an unintended effect on the analysis.\n",
    "\n",
    "The remaining documents discuss musicians, alblums, and performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec12b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 3651\n"
     ]
    }
   ],
   "source": [
    "exclude = ['Film/DVD Review', 'Film/DVD/TV Review',\n",
    "           'Letters', \"Readers' Letters\", \n",
    "           'Obituary', 'Memoir', \n",
    "           'Book Excerpt', 'Book Review',\n",
    "           'Audio transcript of interview']\n",
    "\n",
    "LIB = LIB.loc[~LIB.type.isin(exclude)]\n",
    "print(f\"Number of Articles: {len(LIB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79faf4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c1419</th>\n",
       "      <td>Willie Nelson: Hammersmith Odeon, London</td>\n",
       "      <td>Mick Brown</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>1982-06-09 00:00:00</td>\n",
       "      <td>[willie-nelson]</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=willie-nels...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3713</th>\n",
       "      <td>Al Green, Laura Lee: Apollo Theatre, New York NY</td>\n",
       "      <td>Dan Nooger</td>\n",
       "      <td>The Village Voice</td>\n",
       "      <td>1973-11-15 00:00:00</td>\n",
       "      <td>[al-green, laura-lee]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=al-green-la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11209</th>\n",
       "      <td>M People: Swing Out Citrus</td>\n",
       "      <td>John Harris</td>\n",
       "      <td>New Musical Express</td>\n",
       "      <td>1994-12-10 00:00:00</td>\n",
       "      <td>[m-people]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=m-people-sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s4501</th>\n",
       "      <td>Ohio Players, Graham Central Station, Funkadel...</td>\n",
       "      <td>Vernon Gibbs</td>\n",
       "      <td>The Village Voice</td>\n",
       "      <td>1975-02-24 00:00:00</td>\n",
       "      <td>[funkadelic, ohio-players-the, graham-central-...</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=ohio-player...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c306</th>\n",
       "      <td>Merle Haggard: Home-fried Humor and Cowboy Soul</td>\n",
       "      <td>Al Aronowitz</td>\n",
       "      <td>Rolling Stone</td>\n",
       "      <td>1968-08-10 00:00:00</td>\n",
       "      <td>[merle-haggard]</td>\n",
       "      <td>country</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=merle-hagga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s9913</th>\n",
       "      <td>World Saxophone Quartet: Rhythm 'n Blues (Elek...</td>\n",
       "      <td>Kirk Silsbee</td>\n",
       "      <td>Musician</td>\n",
       "      <td>1989-09-01 00:00:00</td>\n",
       "      <td>[world-saxophone-quartet]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=world-saxop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3316</th>\n",
       "      <td>Dolly Parton: Backwoods Barbie **½</td>\n",
       "      <td>Mark Kemp</td>\n",
       "      <td>Rolling Stone Online</td>\n",
       "      <td>2008-03-06 00:00:00</td>\n",
       "      <td>[dolly-parton]</td>\n",
       "      <td>country</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=dolly-parto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s7617</th>\n",
       "      <td>Fela: Return of the Afrobeat Rebel</td>\n",
       "      <td>Randall Grass</td>\n",
       "      <td>Musician</td>\n",
       "      <td>1983-10-01 00:00:00</td>\n",
       "      <td>[fela-kuti]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Profile</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=fela-return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s10010</th>\n",
       "      <td>A Schism Divides Black Pop Radical Rappers And...</td>\n",
       "      <td>Jim Sullivan</td>\n",
       "      <td>The Boston Globe</td>\n",
       "      <td>1989-12-31 00:00:00</td>\n",
       "      <td>[beastie-boys-the, earth-wind--fire, ll-cool-j...</td>\n",
       "      <td>soul</td>\n",
       "      <td>Comment</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=a-schism-di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1407</th>\n",
       "      <td>Rodney Crowell: A Songwriter Surfaces</td>\n",
       "      <td>Fred Schruers</td>\n",
       "      <td>Rolling Stone</td>\n",
       "      <td>1980-08-21 00:00:00</td>\n",
       "      <td>[rodney-crowell]</td>\n",
       "      <td>country</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=rodney-crow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title         author  \\\n",
       "article_id                                                                     \n",
       "c1419                Willie Nelson: Hammersmith Odeon, London     Mick Brown   \n",
       "s3713        Al Green, Laura Lee: Apollo Theatre, New York NY     Dan Nooger   \n",
       "s11209                             M People: Swing Out Citrus    John Harris   \n",
       "s4501       Ohio Players, Graham Central Station, Funkadel...   Vernon Gibbs   \n",
       "c306          Merle Haggard: Home-fried Humor and Cowboy Soul   Al Aronowitz   \n",
       "s9913       World Saxophone Quartet: Rhythm 'n Blues (Elek...   Kirk Silsbee   \n",
       "c3316                      Dolly Parton: Backwoods Barbie **½      Mark Kemp   \n",
       "s7617                      Fela: Return of the Afrobeat Rebel  Randall Grass   \n",
       "s10010      A Schism Divides Black Pop Radical Rappers And...   Jim Sullivan   \n",
       "c1407                   Rodney Crowell: A Songwriter Surfaces  Fred Schruers   \n",
       "\n",
       "                          source                 date  \\\n",
       "article_id                                              \n",
       "c1419               The Guardian  1982-06-09 00:00:00   \n",
       "s3713          The Village Voice  1973-11-15 00:00:00   \n",
       "s11209       New Musical Express  1994-12-10 00:00:00   \n",
       "s4501          The Village Voice  1975-02-24 00:00:00   \n",
       "c306               Rolling Stone  1968-08-10 00:00:00   \n",
       "s9913                   Musician  1989-09-01 00:00:00   \n",
       "c3316       Rolling Stone Online  2008-03-06 00:00:00   \n",
       "s7617                   Musician  1983-10-01 00:00:00   \n",
       "s10010          The Boston Globe  1989-12-31 00:00:00   \n",
       "c1407              Rolling Stone  1980-08-21 00:00:00   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "c1419                                         [willie-nelson]  country   \n",
       "s3713                                   [al-green, laura-lee]     soul   \n",
       "s11209                                             [m-people]     soul   \n",
       "s4501       [funkadelic, ohio-players-the, graham-central-...     soul   \n",
       "c306                                          [merle-haggard]  country   \n",
       "s9913                               [world-saxophone-quartet]     soul   \n",
       "c3316                                          [dolly-parton]  country   \n",
       "s7617                                             [fela-kuti]     soul   \n",
       "s10010      [beastie-boys-the, earth-wind--fire, ll-cool-j...     soul   \n",
       "c1407                                        [rodney-crowell]  country   \n",
       "\n",
       "                             type  \\\n",
       "article_id                          \n",
       "c1419                 Live Review   \n",
       "s3713                 Live Review   \n",
       "s11209                  Interview   \n",
       "s4501                 Live Review   \n",
       "c306        Profile and Interview   \n",
       "s9913                      Review   \n",
       "c3316                      Review   \n",
       "s7617                     Profile   \n",
       "s10010                    Comment   \n",
       "c1407       Profile and Interview   \n",
       "\n",
       "                                                         href  \n",
       "article_id                                                     \n",
       "c1419       /Library/SearchLinkRedirect?folder=willie-nels...  \n",
       "s3713       /Library/SearchLinkRedirect?folder=al-green-la...  \n",
       "s11209      /Library/SearchLinkRedirect?folder=m-people-sw...  \n",
       "s4501       /Library/SearchLinkRedirect?folder=ohio-player...  \n",
       "c306        /Library/SearchLinkRedirect?folder=merle-hagga...  \n",
       "s9913       /Library/SearchLinkRedirect?folder=world-saxop...  \n",
       "c3316       /Library/SearchLinkRedirect?folder=dolly-parto...  \n",
       "s7617       /Library/SearchLinkRedirect?folder=fela-return...  \n",
       "s10010      /Library/SearchLinkRedirect?folder=a-schism-di...  \n",
       "c1407       /Library/SearchLinkRedirect?folder=rodney-crow...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c5a62",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing the Corpus<a name=\"paragraph3\"></a>\n",
    "I read in the body of each document and tokenize it with `nltk`'s `sent_tokenize()` and `WhitespaceTokenizer()` methods. Tokens are amassed in the `CORPUS` table and indexed by document OHCO structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7908aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for reading files\n",
    "def RBP_reader(filePath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(filePath, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, 'html')\n",
    "    \n",
    "    writer = soup.find(\"span\", class_=\"writer\") \\\n",
    "                 .get_text() \\\n",
    "                 .replace(r'\\n+', ' ') \\\n",
    "                 .strip()\n",
    "    \n",
    "    standfirst = soup.find(\"div\", class_=\"standfirst\").get_text()\n",
    "    copy = soup.find(\"div\", class_=\"copy\").get_text()\n",
    "\n",
    "    # if standfirst contains writer name, it is assumed that \n",
    "    # standfirst is purely metadata and should be ignored\n",
    "    if writer.lower() in standfirst.lower():\n",
    "        doc = copy\n",
    "    else:\n",
    "        doc = standfirst+copy\n",
    "    \n",
    "    return doc.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(library, filePrefix = \"./data/html/\"):\n",
    "    '''\n",
    "    Inputs\n",
    "    -----\n",
    "    library - pandas dataframe, must include author_id as index\n",
    "    fileprefix - string, path to directory containing .html files\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    CORPUS - pandas dataframe, contains all tokens in corpus\n",
    "    '''\n",
    "    import re\n",
    "    import nltk\n",
    "    \n",
    "    para_pat = r\"\\n{2,}\"\n",
    "    documents = []\n",
    "    \n",
    "    i=0\n",
    "    numIter = len(library)\n",
    "    \n",
    "    for id in library.index:\n",
    "        \n",
    "        doc = RBP_reader(filePrefix+str(id)+\".html\")\n",
    "        \n",
    "        PARAS = re.split(para_pat, doc)\n",
    "        PARAS = [x.strip().replace('\\n', ' ') for x in PARAS]\n",
    "        PARAS = pd.DataFrame(PARAS, columns = ['para_str'])\n",
    "        PARAS.index.name = 'para_id'\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame('sent_str')\n",
    "        \n",
    "        SENTS.index.names = ['para_id', 'sent_id']\n",
    "        \n",
    "        SENTS = SENTS.sent_str.str.replace(\"-\", \" \").to_frame()\n",
    "        \n",
    "        TOKENS = SENTS.sent_str\\\n",
    "                      .apply(lambda x: \\\n",
    "                             pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))) \\\n",
    "                      .stack() \\\n",
    "                      .to_frame('pos_tuple')\n",
    "\n",
    "        TOKENS.index.names = OHCO[1:]\n",
    "        \n",
    "        TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "        TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "        TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "        \n",
    "        punc_pos = ['$', \"''\", '(', ')', '[', ']', ',', '--', '.', ':', '``']\n",
    "        TOKENS['term_str'] = TOKENS[~TOKENS.pos.isin(punc_pos)].token_str \\\n",
    "                                .str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS['article_id'] = id\n",
    "        TOKENS = TOKENS.reset_index().set_index(OHCO)\n",
    "        \n",
    "        documents.append(TOKENS)\n",
    "        \n",
    "        if i % round(numIter/5) == 0:\n",
    "            print(f\"{round(i*100/numIter)}% Complete\")\n",
    "        i +=1\n",
    "\n",
    "    \n",
    "    CORPUS = pd.concat(documents).sort_index()\n",
    "    \n",
    "    del(documents)\n",
    "    del(PARAS)\n",
    "    del(SENTS)\n",
    "    del(TOKENS)\n",
    "    \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be5dfb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% Complete\n",
      "20% Complete\n",
      "40% Complete\n",
      "60% Complete\n",
      "80% Complete\n",
      "100% Complete\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "689194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS.to_csv('./data/CORPUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3a2c791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s5313</th>\n",
       "      <th>50</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>(Over, IN)</td>\n",
       "      <td>IN</td>\n",
       "      <td>Over</td>\n",
       "      <td>over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2218</th>\n",
       "      <th>3</th>\n",
       "      <th>3</th>\n",
       "      <th>8</th>\n",
       "      <td>(disparities, NNS)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>disparities</td>\n",
       "      <td>disparities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s10500</th>\n",
       "      <th>8</th>\n",
       "      <th>2</th>\n",
       "      <th>26</th>\n",
       "      <td>(fearful., NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>fearful.</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2118</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>8</th>\n",
       "      <td>(of, IN)</td>\n",
       "      <td>IN</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3806</th>\n",
       "      <th>10</th>\n",
       "      <th>2</th>\n",
       "      <th>18</th>\n",
       "      <td>(ground, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>ground</td>\n",
       "      <td>ground</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1614</th>\n",
       "      <th>32</th>\n",
       "      <th>2</th>\n",
       "      <th>36</th>\n",
       "      <td>(country, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s8003</th>\n",
       "      <th>12</th>\n",
       "      <th>2</th>\n",
       "      <th>13</th>\n",
       "      <td>(some, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11716</th>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <th>6</th>\n",
       "      <td>(explosions,, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>explosions,</td>\n",
       "      <td>explosions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2803</th>\n",
       "      <th>22</th>\n",
       "      <th>0</th>\n",
       "      <th>8</th>\n",
       "      <td>(does, VBZ)</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>does</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s9405</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>46</th>\n",
       "      <td>(singing, VBG)</td>\n",
       "      <td>VBG</td>\n",
       "      <td>singing</td>\n",
       "      <td>singing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              pos_tuple  pos    token_str  \\\n",
       "article_id para_id sent_id token_id                                         \n",
       "s5313      50      0       0                 (Over, IN)   IN         Over   \n",
       "c2218      3       3       8         (disparities, NNS)  NNS  disparities   \n",
       "s10500     8       2       26            (fearful., NN)   NN     fearful.   \n",
       "c2118      1       0       8                   (of, IN)   IN           of   \n",
       "c3806      10      2       18              (ground, NN)   NN       ground   \n",
       "c1614      32      2       36             (country, NN)   NN      country   \n",
       "s8003      12      2       13                (some, DT)   DT         some   \n",
       "s11716     7       0       6          (explosions,, NN)   NN  explosions,   \n",
       "s2803      22      0       8                (does, VBZ)  VBZ         does   \n",
       "s9405      2       0       46            (singing, VBG)  VBG      singing   \n",
       "\n",
       "                                        term_str  \n",
       "article_id para_id sent_id token_id               \n",
       "s5313      50      0       0                over  \n",
       "c2218      3       3       8         disparities  \n",
       "s10500     8       2       26            fearful  \n",
       "c2118      1       0       8                  of  \n",
       "c3806      10      2       18             ground  \n",
       "c1614      32      2       36            country  \n",
       "s8003      12      2       13               some  \n",
       "s11716     7       0       6          explosions  \n",
       "s2803      22      0       8                does  \n",
       "s9405      2       0       46            singing  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833cd37",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting a Vocabulary<a name=\"paragraph4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1fc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
