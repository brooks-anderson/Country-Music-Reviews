{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c204d",
   "metadata": {},
   "source": [
    "# Country Music Reviews\n",
    "### *Digital Analytical Edition*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19f2c5",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries & Set Up](#paragraph1)\n",
    "3. [Creating the LIB table](#paragraph2)\n",
    "    1. [Parsing Dates](#subparagraph1)\n",
    "4. [Constructing the Corpus](#paragraph3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330874",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>\n",
    "In this notebook, I aggregate the source files to create a Digital Analytical Edition (DAE). Source files were scraped from [rocksbackpage.com](rocksbackpage.com) using `RBP_search_scraper.py` and `RBP_article_scraper.py`.  \n",
    "\n",
    "The DAE contains the following tables:  \n",
    "+ `LIB.csv`    - Metadata for each article.\n",
    "+ `CORPUS.csv` - Aggregated text from source files. Indexed by Ordered Hierarchy of Content Object (OHCO) structure.\n",
    "+ `VOCAB.csv`  - Linguistic features and statistics for words in `CORPUS.csv`. Stop words removed.\n",
    "+ `BOW.csv`    - Bag-of-words representation of `CORPUS.csv` with stop words removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cbd2d",
   "metadata": {},
   "source": [
    "## Libraries & Set Up<a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa0e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "txtPath = \"./data/articles_txt/\"\n",
    "htmlPath = \"./data/articles_html/\"\n",
    "\n",
    "# define OHCO structure\n",
    "OHCO = ['article_id', 'para_id', 'sent_id', 'token_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f61b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Library Table<a name=\"paragraph2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff00eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in article & search metadata\n",
    "article_metadata = pd.read_csv(\"./data/RBP_article_metadata.csv\")\n",
    "article_metadata.set_index(\"id\", inplace = True)\n",
    "\n",
    "search_metadata =pd.read_csv(\"./data/RBP_search_metadata.csv\")\n",
    "search_metadata.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e86e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = article_metadata\n",
    "LIB.index.rename(OHCO[0], inplace = True)\n",
    "\n",
    "LIB['doc_type'] = search_metadata.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b231",
   "metadata": {},
   "source": [
    "#### Parsing Dates<a name=\"subparagraph1\"></a>\n",
    "Inspection reveals that the publication dates are not in a standard format. I define a function that parses date strings. The following table describes how dates are assigned to ambiguous formats.\n",
    "\n",
    "|`date` format|output format|\n",
    "|------------|-------------|\n",
    "|Month *year*|YYYY-mm-01|\n",
    "|*year*|YYYY-07-01|\n",
    "|Summer *year*|YYYY-08-01|\n",
    "|Fall *year*|YYYY-11-01|\n",
    "|Winter *year*|YYYY-02-01|\n",
    "|Spring *year*|YYYY-05-01|\n",
    "\n",
    "Any date string that is missing or does not adhere to one of the above formats will be corrected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_date(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%d %B %Y\")\n",
    "\n",
    "def year_only(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dt_obj = datetime.strptime(dt, \"%Y\").date()\n",
    "    return dt_obj.replace(month = 7, day = 1)\n",
    "\n",
    "def month_year(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%B %Y\").date()\n",
    "\n",
    "def season_year(dt):\n",
    "    from datetime import datetime, date\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        season = dt.split()[-2].lower()\n",
    "        year = dt.split()[-1]\n",
    "\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "    \n",
    "    if season == 'spring':\n",
    "        return date(int(year), 5, 1)\n",
    "    elif season == 'summer':\n",
    "        return date(int(year), 8, 1)\n",
    "    elif season == 'fall':\n",
    "        return date(int(year), 11, 1)\n",
    "    elif season == 'winter':\n",
    "        return date(int(year), 2, 1)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(dt):\n",
    "    '''\n",
    "    Converts publication date strings from rocksbackpages.com articles\n",
    "    to date objects with format YYYY-mm-dd.\n",
    "    '''\n",
    "    helpers = [normal_date, year_only, month_year, season_year]\n",
    "    \n",
    "    for f in helpers:\n",
    "        try:\n",
    "            return f(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['date_parsed'] = LIB.date.apply(date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4a1692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Charley Pride, Tammy Wynette and George Jones:...</td>\n",
       "      <td>Gene Guerrero</td>\n",
       "      <td>The Great Speckled Bird</td>\n",
       "      <td>31 February 1972</td>\n",
       "      <td>['george-jones', 'tammy-wynette', 'charley-pri...</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title         author  \\\n",
       "article_id                                                                     \n",
       "610         Charley Pride, Tammy Wynette and George Jones:...  Gene Guerrero   \n",
       "\n",
       "                             source              date  \\\n",
       "article_id                                              \n",
       "610         The Great Speckled Bird  31 February 1972   \n",
       "\n",
       "                                                     subjects     doc_type  \\\n",
       "article_id                                                                   \n",
       "610         ['george-jones', 'tammy-wynette', 'charley-pri...  Live Review   \n",
       "\n",
       "           date_parsed  \n",
       "article_id              \n",
       "610                NaT  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.loc[LIB.date_parsed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0faab7",
   "metadata": {},
   "source": [
    "Only one date failed to parse. We will manually set this date to February 29, 1972. After that, we will replace the `date` column with `date_parsed` as keeping both is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4121cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "LIB.loc[610, 'date_parsed'] = date(1972,2,29)\n",
    "\n",
    "# replace date col\n",
    "LIB['date'] = LIB.date_parsed\n",
    "LIB.drop(columns ='date_parsed', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d185",
   "metadata": {},
   "source": [
    "Next, I notice that the `subjects` column is a string representation of a list--not an actual list. `subjects` is converted to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37dfe556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "LIB.subjects = LIB.subjects.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01067c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>doc_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>Classic Rockers Firefall Drop First New Record...</td>\n",
       "      <td>Bob Ruggiero</td>\n",
       "      <td>Houston Press</td>\n",
       "      <td>2020-11-30 00:00:00</td>\n",
       "      <td>[firefall, gram-parsons]</td>\n",
       "      <td>Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>Linda Ronstadt: 'Heat Wave' – The Long Hot Ses...</td>\n",
       "      <td>Todd Everett</td>\n",
       "      <td>Rolling Stone</td>\n",
       "      <td>1975-12-18 00:00:00</td>\n",
       "      <td>[linda-ronstadt]</td>\n",
       "      <td>Report and Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>George Jones, August 1976, at Sunset Park, Wes...</td>\n",
       "      <td>Peter Stone Brown</td>\n",
       "      <td>unpublished</td>\n",
       "      <td>1976-08-01 00:00:00</td>\n",
       "      <td>[george-jones]</td>\n",
       "      <td>Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>Sandy Posey</td>\n",
       "      <td>Phil Hardy, Dave Laing</td>\n",
       "      <td>The Faber Companion to 20th-Century Popular Music</td>\n",
       "      <td>2001-07-01 00:00:00</td>\n",
       "      <td>[sandy-posey]</td>\n",
       "      <td>Book Excerpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Linda Leading Stone Poneys To Gold Water</td>\n",
       "      <td>Tony Leigh</td>\n",
       "      <td>KRLA Beat</td>\n",
       "      <td>1967-12-30 00:00:00</td>\n",
       "      <td>[stone-poneys-the]</td>\n",
       "      <td>Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>Laura Cantrell: St. Bonaventure's, Bristol</td>\n",
       "      <td>Stephen Dalton</td>\n",
       "      <td>The Times</td>\n",
       "      <td>2011-05-04 00:00:00</td>\n",
       "      <td>[laura-cantrell]</td>\n",
       "      <td>Live Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>Shelby Lynne: I Am Shelby Lynne (Mercury)</td>\n",
       "      <td>Tom Cox</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>1999-09-24 00:00:00</td>\n",
       "      <td>[shelby-lynne]</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>Travis Tritt: Ten Feet Tall and Bulletproof (W...</td>\n",
       "      <td>Eric Weisbard</td>\n",
       "      <td>Spin</td>\n",
       "      <td>1994-06-01 00:00:00</td>\n",
       "      <td>[travis-tritt]</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Lynn Anderson: Pssst! Don't tell the British t...</td>\n",
       "      <td>Richard Green</td>\n",
       "      <td>New Musical Express</td>\n",
       "      <td>1971-03-20 00:00:00</td>\n",
       "      <td>[lynn-anderson]</td>\n",
       "      <td>Interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>Rosanne Cash: Somewhere In The Stars (Columbia)</td>\n",
       "      <td>Mitchell Cohen</td>\n",
       "      <td>Creem</td>\n",
       "      <td>1982-10-01 00:00:00</td>\n",
       "      <td>[rosanne-cash]</td>\n",
       "      <td>Review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "4115        Classic Rockers Firefall Drop First New Record...   \n",
       "1019        Linda Ronstadt: 'Heat Wave' – The Long Hot Ses...   \n",
       "1118        George Jones, August 1976, at Sunset Park, Wes...   \n",
       "2602                                              Sandy Posey   \n",
       "216                  Linda Leading Stone Poneys To Gold Water   \n",
       "3601               Laura Cantrell: St. Bonaventure's, Bristol   \n",
       "2503                Shelby Lynne: I Am Shelby Lynne (Mercury)   \n",
       "2103        Travis Tritt: Ten Feet Tall and Bulletproof (W...   \n",
       "508         Lynn Anderson: Pssst! Don't tell the British t...   \n",
       "1502          Rosanne Cash: Somewhere In The Stars (Columbia)   \n",
       "\n",
       "                            author  \\\n",
       "article_id                           \n",
       "4115                  Bob Ruggiero   \n",
       "1019                  Todd Everett   \n",
       "1118             Peter Stone Brown   \n",
       "2602        Phil Hardy, Dave Laing   \n",
       "216                     Tony Leigh   \n",
       "3601                Stephen Dalton   \n",
       "2503                       Tom Cox   \n",
       "2103                 Eric Weisbard   \n",
       "508                  Richard Green   \n",
       "1502                Mitchell Cohen   \n",
       "\n",
       "                                                       source  \\\n",
       "article_id                                                      \n",
       "4115                                            Houston Press   \n",
       "1019                                            Rolling Stone   \n",
       "1118                                              unpublished   \n",
       "2602        The Faber Companion to 20th-Century Popular Music   \n",
       "216                                                 KRLA Beat   \n",
       "3601                                                The Times   \n",
       "2503                                             The Guardian   \n",
       "2103                                                     Spin   \n",
       "508                                       New Musical Express   \n",
       "1502                                                    Creem   \n",
       "\n",
       "                           date                  subjects  \\\n",
       "article_id                                                  \n",
       "4115        2020-11-30 00:00:00  [firefall, gram-parsons]   \n",
       "1019        1975-12-18 00:00:00          [linda-ronstadt]   \n",
       "1118        1976-08-01 00:00:00            [george-jones]   \n",
       "2602        2001-07-01 00:00:00             [sandy-posey]   \n",
       "216         1967-12-30 00:00:00        [stone-poneys-the]   \n",
       "3601        2011-05-04 00:00:00          [laura-cantrell]   \n",
       "2503        1999-09-24 00:00:00            [shelby-lynne]   \n",
       "2103        1994-06-01 00:00:00            [travis-tritt]   \n",
       "508         1971-03-20 00:00:00           [lynn-anderson]   \n",
       "1502        1982-10-01 00:00:00            [rosanne-cash]   \n",
       "\n",
       "                        doc_type  \n",
       "article_id                        \n",
       "4115                   Interview  \n",
       "1019        Report and Interview  \n",
       "1118                   Interview  \n",
       "2602                Book Excerpt  \n",
       "216                    Interview  \n",
       "3601                 Live Review  \n",
       "2503                      Review  \n",
       "2103                      Review  \n",
       "508                    Interview  \n",
       "1502                      Review  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview LIB\n",
    "LIB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c5a62",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing the Corpus<a name=\"paragraph3\"></a>\n",
    "I read in the body of each document and tokenize it with scikit-learn's `CountVectorizer`. Tokens are amassed in the `CORPUS` table and indexed by document OHCO structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7908aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for reading files\n",
    "def RBP_reader(filePath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(filePath, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, 'html')\n",
    "    \n",
    "    writer = soup.find(\"span\", class_=\"writer\") \\\n",
    "                 .get_text() \\\n",
    "                 .replace(r'\\n+', ' ') \\\n",
    "                 .strip()\n",
    "    \n",
    "    standfirst = soup.find(\"div\", class_=\"standfirst\").get_text()\n",
    "    copy = soup.find(\"div\", class_=\"copy\").get_text()\n",
    "\n",
    "    # if standfirst contains writer name, it is assumed that \n",
    "    # standfirst is purely metadata and should be ignored\n",
    "    if writer.lower() in standfirst.lower():\n",
    "        doc = copy\n",
    "    else:\n",
    "        doc = standfirst+copy\n",
    "    \n",
    "    return doc.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(library, filePrefix = \"./data/articles_html/\"):\n",
    "    '''\n",
    "    Inputs\n",
    "    -----\n",
    "    library - pandas dataframe, must include author_id as index\n",
    "    fileprefix - string, path to directory containing .html files\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    CORPUS - pandas dataframe, contains all tokens in corpus\n",
    "    '''\n",
    "    import re\n",
    "    import nltk\n",
    "    \n",
    "    para_pat = r\"\\n{2,}\"\n",
    "    documents = []\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for id in library.index:\n",
    "        \n",
    "        doc = RBP_reader(filePrefix+str(id)+\".html\")\n",
    "        \n",
    "        PARAS = re.split(para_pat, doc)\n",
    "        PARAS = [x.strip().replace('\\n', ' ') for x in PARAS]\n",
    "        PARAS = pd.DataFrame(PARAS, columns = ['para_str'])\n",
    "        PARAS.index.name = 'para_id'\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame('sent_str')\n",
    "        \n",
    "        SENTS.index.names = ['para_id', 'sent_id']\n",
    "        \n",
    "        SENTS = SENTS.sent_str.str.replace(\"-\", \" \").to_frame()\n",
    "        \n",
    "        TOKENS = SENTS.sent_str\\\n",
    "                      .apply(lambda x: \\\n",
    "                             pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))) \\\n",
    "                      .stack() \\\n",
    "                      .to_frame('pos_tuple')\n",
    "\n",
    "        TOKENS.index.names = OHCO[1:]\n",
    "        \n",
    "        TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "        TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "        TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "        \n",
    "        punc_pos = ['$', \"''\", '(', ')', '[', ']', ',', '--', '.', ':', '``']\n",
    "        TOKENS['term_str'] = TOKENS[~TOKENS.pos.isin(punc_pos)].token_str \\\n",
    "                                .str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS['article_id'] = id\n",
    "        TOKENS = TOKENS.reset_index().set_index(OHCO)\n",
    "        \n",
    "        documents.append(TOKENS)\n",
    "        \n",
    "        i +=1\n",
    "        if i % 150 == 0:\n",
    "            print(f\"{round(i/len(library.index)*100, 1)}% Complete\")\n",
    "    \n",
    "    CORPUS = pd.concat(documents).sort_index()\n",
    "    \n",
    "    del(documents)\n",
    "    del(PARAS)\n",
    "    del(SENTS)\n",
    "    del(TOKENS)\n",
    "    \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be5dfb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6% Complete\n",
      "37.3% Complete\n",
      "55.9% Complete\n",
      "74.5% Complete\n",
      "93.2% Complete\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS.to_csv('./data/CORPUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6dee01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th>para_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <th>8</th>\n",
       "      <th>2</th>\n",
       "      <th>12</th>\n",
       "      <td>(Nelson's, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Nelson's</td>\n",
       "      <td>nelsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <th>7</th>\n",
       "      <th>2</th>\n",
       "      <th>16</th>\n",
       "      <td>(ever, RB)</td>\n",
       "      <td>RB</td>\n",
       "      <td>ever</td>\n",
       "      <td>ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <th>20</th>\n",
       "      <td>(old, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>old</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <th>69</th>\n",
       "      <th>0</th>\n",
       "      <th>11</th>\n",
       "      <td>(meaning,, NNS)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>meaning,</td>\n",
       "      <td>meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <th>13</th>\n",
       "      <th>1</th>\n",
       "      <th>16</th>\n",
       "      <td>(she'll, JJ)</td>\n",
       "      <td>JJ</td>\n",
       "      <td>she'll</td>\n",
       "      <td>shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3101</th>\n",
       "      <th>29</th>\n",
       "      <th>10</th>\n",
       "      <th>0</th>\n",
       "      <td>(Don't, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Don't</td>\n",
       "      <td>dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3019</th>\n",
       "      <th>21</th>\n",
       "      <th>1</th>\n",
       "      <th>41</th>\n",
       "      <td>(plot, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>plot</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <th>11</th>\n",
       "      <th>1</th>\n",
       "      <th>8</th>\n",
       "      <td>(Kennedy,, NNP)</td>\n",
       "      <td>NNP</td>\n",
       "      <td>Kennedy,</td>\n",
       "      <td>kennedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <th>51</th>\n",
       "      <th>0</th>\n",
       "      <th>18</th>\n",
       "      <td>(punk, NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>punk</td>\n",
       "      <td>punk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <th>25</th>\n",
       "      <th>2</th>\n",
       "      <th>23</th>\n",
       "      <td>(too, RB)</td>\n",
       "      <td>RB</td>\n",
       "      <td>too</td>\n",
       "      <td>too</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           pos_tuple  pos token_str term_str\n",
       "article_id para_id sent_id token_id                                         \n",
       "1207       8       2       12        (Nelson's, NNP)  NNP  Nelson's  nelsons\n",
       "3014       7       2       16             (ever, RB)   RB      ever     ever\n",
       "3904       4       1       20              (old, JJ)   JJ       old      old\n",
       "1002       69      0       11        (meaning,, NNS)  NNS  meaning,  meaning\n",
       "1815       13      1       16           (she'll, JJ)   JJ    she'll    shell\n",
       "3101       29      10      0            (Don't, NNP)  NNP     Don't     dont\n",
       "3019       21      1       41             (plot, NN)   NN      plot     plot\n",
       "3609       11      1       8         (Kennedy,, NNP)  NNP  Kennedy,  kennedy\n",
       "2912       51      0       18             (punk, NN)   NN      punk     punk\n",
       "4018       25      2       23              (too, RB)   RB       too      too"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7b147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
