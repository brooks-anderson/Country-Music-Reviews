{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c204d",
   "metadata": {},
   "source": [
    "# Country & Soul\n",
    "### *Analyzing Trends in American Music Journalism from 1960 to Present*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19f2c5",
   "metadata": {},
   "source": [
    "# Digital Analytical Edition\n",
    "#### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries & Set Up](#paragraph1)\n",
    "3. [Creating the LIB table](#paragraph2)\n",
    "    1. [Parsing Dates](#subparagraph1)\n",
    "    2. [Narrowing the Scope](#subparagraph2)\n",
    "4. [Constructing the Corpus](#paragraph3)\n",
    "5. [Extracting a Vocabulary](#paragraph4)\n",
    "6. [Creating a Bag-of-Words](#paragraph5)\n",
    "    1. [Add Frequency Features to `VOCAB`](#subparagraph3)\n",
    "7. [Saving the Digital Analytic Edition](#paragraph6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330874",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>\n",
    "In this notebook, I aggregate and parse the source files to create a Digital Analytical Edition (DAE).  \n",
    "\n",
    "The DAE contains the following tables:  \n",
    "+ `LIB.csv`    - Metadata for each article.\n",
    "+ `CORPUS.csv` - Aggregated text from source files. Indexed by Ordered Hierarchy of Content Object (OHCO) structure.\n",
    "+ `VOCAB.csv`  - Linguistic features and statistics for words in `CORPUS.csv`. Stop words removed.\n",
    "+ `BOW.csv`    - Bag-of-words representation of `CORPUS.csv` with stop words removed.\n",
    "\n",
    "Source files were scraped from [rocksbackpages.com](https://www.rocksbackpages.com/) using `rbpscraper.py`. I present an example of `rbpscraper.py` functionality here. It is **not** recommended that the reader attempts to run this code. `rbpscraper.py` requires a subscription to rocksbackpages.com and several hours of your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3feb3be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste URL:\n",
      "https://www.rocksbackpages.com/\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Please paste cookies:\n",
      "sample=cookies; auth=details\n",
      "\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from rbpscraper import RBPScraper\n",
    "\n",
    "country = RBPScraper(desc = \"country\", write_path = \"./datadir/\")\n",
    "    # desc - description of articles to be scraped\n",
    "    # write_path - path to directory where articles will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ef282",
   "metadata": {},
   "source": [
    "After instantiating an RBPScraper object, the user will be prompted to enter the url for the first page of search results. Then, the user will be prompted to enter cookies for authentification. The web-scraping process can be initiated by calling the following methods. This will save articles as html files and a metadata table as a csv file to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd079578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country.searchScraper().articleScraper().writeLIB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cbd2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries & Set Up<a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa0e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "dataPath = \"./data/\"\n",
    "articlePath = \"./data/html/\"\n",
    "\n",
    "# define OHCO structure\n",
    "OHCO = ['article_id', 'para_id', 'sent_id', 'token_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f61b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Library Table<a name=\"paragraph2\"></a>\n",
    "Two library tables were created during the webscraping process. I combine, process, and refine the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in metadata table for each genre\n",
    "country_metadata = pd.read_csv(f\"{dataPath}cLIB.csv\")\n",
    "country_metadata.set_index(\"id\", inplace = True)\n",
    "\n",
    "soul_metadata = pd.read_csv(f\"{dataPath}sLIB.csv\")\n",
    "soul_metadata.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e86e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.concat([country_metadata, soul_metadata])\n",
    "\n",
    "LIB.index.rename(OHCO[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b231",
   "metadata": {},
   "source": [
    "#### Parsing Dates<a name=\"subparagraph1\"></a>\n",
    "Inspection reveals that the publication dates are not in a standard format. I define a function that parses date strings. The following table describes how dates are assigned to ambiguous formats.\n",
    "\n",
    "|`date` format|output format|\n",
    "|------------|-------------|\n",
    "|Month *year*|YYYY-mm-01|\n",
    "|*year*|YYYY-07-01|\n",
    "|Summer *year*|YYYY-08-01|\n",
    "|Fall *year*|YYYY-11-01|\n",
    "|Winter *year*|YYYY-02-01|\n",
    "|Spring *year*|YYYY-05-01|\n",
    "\n",
    "Any date string that is missing or does not adhere to one of the above formats will be corrected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_date(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%d %B %Y\")\n",
    "\n",
    "def year_only(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dt_obj = datetime.strptime(dt, \"%Y\").date()\n",
    "    return dt_obj.replace(month = 7, day = 1)\n",
    "\n",
    "def month_year(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%B %Y\").date()\n",
    "\n",
    "def season_year(dt):\n",
    "    from datetime import datetime, date\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        season = dt.split()[-2].lower()\n",
    "        year = dt.split()[-1]\n",
    "\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "    \n",
    "    if season == 'spring':\n",
    "        return date(int(year), 5, 1)\n",
    "    elif season == 'summer':\n",
    "        return date(int(year), 8, 1)\n",
    "    elif season == 'fall':\n",
    "        return date(int(year), 11, 1)\n",
    "    elif season == 'winter':\n",
    "        return date(int(year), 2, 1)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(dt):\n",
    "    '''\n",
    "    Converts publication date strings from rocksbackpages.com articles\n",
    "    to date objects with format YYYY-mm-dd.\n",
    "    '''\n",
    "    helpers = [normal_date, year_only, month_year, season_year]\n",
    "    \n",
    "    for f in helpers:\n",
    "        try:\n",
    "            return f(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['date_parsed'] = LIB.date.apply(date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4a1692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c610</th>\n",
       "      <td>Charley Pride, Tammy Wynette and George Jones:...</td>\n",
       "      <td>Gene Guerrero</td>\n",
       "      <td>The Great Speckled Bird</td>\n",
       "      <td>31 February 1972</td>\n",
       "      <td>['george-jones', 'tammy-wynette', 'charley-pri...</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=charley-pri...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12801</th>\n",
       "      <td>Duffy: Rockferry</td>\n",
       "      <td>Gavin Martin</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>29 February 2002</td>\n",
       "      <td>['duffy']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=duffy-irock...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12916</th>\n",
       "      <td>Earth Wind And Fire: Beacon Theater, New York</td>\n",
       "      <td>Kandia Crazy Horse</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>15 2003</td>\n",
       "      <td>['earth-wind--fire']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=earth-wind-...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "c610        Charley Pride, Tammy Wynette and George Jones:...   \n",
       "s12801                                       Duffy: Rockferry   \n",
       "s12916          Earth Wind And Fire: Beacon Theater, New York   \n",
       "\n",
       "                        author                   source              date  \\\n",
       "article_id                                                                  \n",
       "c610             Gene Guerrero  The Great Speckled Bird  31 February 1972   \n",
       "s12801            Gavin Martin             Daily Mirror  29 February 2002   \n",
       "s12916      Kandia Crazy Horse               PopMatters           15 2003   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "c610        ['george-jones', 'tammy-wynette', 'charley-pri...  country   \n",
       "s12801                                              ['duffy']     soul   \n",
       "s12916                                   ['earth-wind--fire']     soul   \n",
       "\n",
       "                   type                                               href  \\\n",
       "article_id                                                                   \n",
       "c610        Live Review  /Library/SearchLinkRedirect?folder=charley-pri...   \n",
       "s12801           Review  /Library/SearchLinkRedirect?folder=duffy-irock...   \n",
       "s12916      Live Review  /Library/SearchLinkRedirect?folder=earth-wind-...   \n",
       "\n",
       "           date_parsed  \n",
       "article_id              \n",
       "c610               NaT  \n",
       "s12801             NaT  \n",
       "s12916             NaT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.loc[LIB.date_parsed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0faab7",
   "metadata": {},
   "source": [
    "Only three dates failed to parse. I will manually correct them. After that, I will replace the `date` column with `date_parsed` as keeping both is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4121cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.loc['c610', 'date_parsed'] = date(1972, 2, 29)\n",
    "LIB.loc[\"s12801\", 'date_parsed'] = date(2002, 2, 28)\n",
    "LIB.loc[\"s12916\", 'date_parsed'] = date(2003, 7, 1)\n",
    "\n",
    "# replace date col\n",
    "LIB['date'] = LIB.date_parsed\n",
    "LIB.drop(columns ='date_parsed', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d185",
   "metadata": {},
   "source": [
    "Next, I notice that the `subjects` column is a string representation of a list--not an actual list. `subjects` is converted to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37dfe556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "LIB.subjects = LIB.subjects.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa665f",
   "metadata": {},
   "source": [
    "#### Narrowing the Scope<a name=\"subparagraph2\"></a>\n",
    "I refine the Focus of my analysis rather than work with all 4000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5861589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 articles missing a subject were dropped\n"
     ]
    }
   ],
   "source": [
    "# drop articles not tagged with a subject\n",
    "empty_subjects = [True if x == [] else False for x in LIB.subjects ]\n",
    "print(f\"{sum(empty_subjects)} articles missing a subject were dropped\")\n",
    "\n",
    "LIB = LIB[[not x for x in empty_subjects]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interview</th>\n",
       "      <th>Review</th>\n",
       "      <th>Live Review</th>\n",
       "      <th>Profile and Interview</th>\n",
       "      <th>Report and Interview</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Report</th>\n",
       "      <th>Book Excerpt</th>\n",
       "      <th>Obituary</th>\n",
       "      <th>Retrospective</th>\n",
       "      <th>...</th>\n",
       "      <th>Review and Interview</th>\n",
       "      <th>Film/DVD/TV Review</th>\n",
       "      <th>Guide</th>\n",
       "      <th>Audio transcript of interview</th>\n",
       "      <th>Discography</th>\n",
       "      <th>Special Feature</th>\n",
       "      <th>Readers' Letters</th>\n",
       "      <th>Column</th>\n",
       "      <th>Letters</th>\n",
       "      <th>Film/DVD Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>1016</td>\n",
       "      <td>905</td>\n",
       "      <td>762</td>\n",
       "      <td>301</td>\n",
       "      <td>155</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Interview  Review  Live Review  Profile and Interview  \\\n",
       "type       1016     905          762                    301   \n",
       "\n",
       "      Report and Interview  Profile  Report  Book Excerpt  Obituary  \\\n",
       "type                   155      109     106           104       101   \n",
       "\n",
       "      Retrospective  ...  Review and Interview  Film/DVD/TV Review  Guide  \\\n",
       "type             88  ...                    18                  16      6   \n",
       "\n",
       "      Audio transcript of interview  Discography  Special Feature  \\\n",
       "type                              5            5                4   \n",
       "\n",
       "      Readers' Letters  Column  Letters  Film/DVD Review  \n",
       "type                 3       1        1                1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refine article types\n",
    "\n",
    "# consolidate 'Sleeve and programme notes' with 'Sleevenotes'\n",
    "LIB.loc[LIB.type == 'Sleeve and programme notes', 'type'] = 'Sleevenotes'\n",
    "\n",
    "LIB.type.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd0ba5",
   "metadata": {},
   "source": [
    "Immediately, I see some document types that should be excluded: *Film/DVD Review*, *Letters*, *Readers' Letters*. These documents do not fit with the broader corpus as they are either not about music, or not written by professional journalists.  \n",
    "\n",
    "It is debatable whether *obituary* and *memoir* should be included. These document types are dissimilar from other documents in the corpus as they contain more biographical language. However, if an artist has an associated *obituary* or *memoir*, they were likely to have had significant cultural impact. This analysis is conducted at the level of genre rather than artist. Thus, I discard *obituary* and *memoir* in favor of reducing the size of the corpus.\n",
    "\n",
    "Lastly, I reason that *book excerpts* should also be removed. Out of the 104 book excerpts, 97 are sourced from a single book, *The Faber Companion to 20<sup>th</sup> Century Popular Music*. These excerpts suffer the same issues of unsuitability as the obituaries; they are more pragmatic than they are poetic. Additionally, including so many documents from a single source would almost certainly have an unintended effect on the analysis.\n",
    "\n",
    "The remaining documents discuss musicians, alblums, and performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec12b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 3651\n"
     ]
    }
   ],
   "source": [
    "exclude = ['Film/DVD Review', 'Film/DVD/TV Review',\n",
    "           'Letters', \"Readers' Letters\", \n",
    "           'Obituary', 'Memoir', \n",
    "           'Book Excerpt', 'Book Review',\n",
    "           'Audio transcript of interview']\n",
    "\n",
    "LIB = LIB.loc[~LIB.type.isin(exclude)]\n",
    "print(f\"Number of Articles: {len(LIB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79faf4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s11916</th>\n",
       "      <td>Chaka Khan: Fighting to Reclaim Her Crown</td>\n",
       "      <td>Marc Weingarten</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>1998-10-17 00:00:00</td>\n",
       "      <td>[chaka-khan, prince]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=chaka-khan-...</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2512</th>\n",
       "      <td>Buddy Miles: Speakeasy, London</td>\n",
       "      <td>Chris Welch</td>\n",
       "      <td>Melody Maker</td>\n",
       "      <td>1971-05-22 00:00:00</td>\n",
       "      <td>[buddy-miles]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=buddy-miles...</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s4315</th>\n",
       "      <td>Going Below The Belt With Rufus</td>\n",
       "      <td>David Hancock</td>\n",
       "      <td>Record Mirror</td>\n",
       "      <td>1974-12-14 00:00:00</td>\n",
       "      <td>[rufus]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=going-below...</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s313</th>\n",
       "      <td>From Pop Singers To Rock Bands</td>\n",
       "      <td>Geoffrey Cannon</td>\n",
       "      <td>unpublished</td>\n",
       "      <td>1965-07-01 00:00:00</td>\n",
       "      <td>[chuck-berry, ray-charles, billie-holiday, rol...</td>\n",
       "      <td>soul</td>\n",
       "      <td>Essay</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=from-pop-si...</td>\n",
       "      <td>2226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11701</th>\n",
       "      <td>New Edition, Keith Sweat, BLACKstreet: Madison...</td>\n",
       "      <td>Amy Linden</td>\n",
       "      <td>New York Daily News</td>\n",
       "      <td>1997-01-13 00:00:00</td>\n",
       "      <td>[new-edition, blackstreet, keith-sweat]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=new-edition...</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1710</th>\n",
       "      <td>Lyle Lovett: Lovett and Leave It</td>\n",
       "      <td>Adam Sweeting</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>1988-03-04 00:00:00</td>\n",
       "      <td>[lyle-lovett]</td>\n",
       "      <td>country</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=lyle-lovett...</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s5512</th>\n",
       "      <td>Boney M: Take The Heat Off Me (Atlantic K50314...</td>\n",
       "      <td>Davitt Sigerson</td>\n",
       "      <td>Black Music</td>\n",
       "      <td>1977-02-01 00:00:00</td>\n",
       "      <td>[boney-m]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=boney-m-ita...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12100</th>\n",
       "      <td>Faith Evans, Dru Hill: Universal Amphitheatre,...</td>\n",
       "      <td>Marc Weingarten</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>1999-04-19 00:00:00</td>\n",
       "      <td>[dru-hill, faith-evans]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=faith-evans...</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s7118</th>\n",
       "      <td>Talking Heads, Defunkt: Greek Theater, Los Ang...</td>\n",
       "      <td>Don Waller</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>1982-08-10 00:00:00</td>\n",
       "      <td>[defunkt, talking-heads]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=talking-hea...</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3603</th>\n",
       "      <td>Frank Fairfield: Out On The Open West</td>\n",
       "      <td>Rob Hughes</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>2011-08-01 00:00:00</td>\n",
       "      <td>[frank-fairfield]</td>\n",
       "      <td>country</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=frank-fairf...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "s11916              Chaka Khan: Fighting to Reclaim Her Crown   \n",
       "s2512                          Buddy Miles: Speakeasy, London   \n",
       "s4315                         Going Below The Belt With Rufus   \n",
       "s313                           From Pop Singers To Rock Bands   \n",
       "s11701      New Edition, Keith Sweat, BLACKstreet: Madison...   \n",
       "c1710                        Lyle Lovett: Lovett and Leave It   \n",
       "s5512       Boney M: Take The Heat Off Me (Atlantic K50314...   \n",
       "s12100      Faith Evans, Dru Hill: Universal Amphitheatre,...   \n",
       "s7118       Talking Heads, Defunkt: Greek Theater, Los Ang...   \n",
       "c3603                   Frank Fairfield: Out On The Open West   \n",
       "\n",
       "                     author               source                 date  \\\n",
       "article_id                                                              \n",
       "s11916      Marc Weingarten    Los Angeles Times  1998-10-17 00:00:00   \n",
       "s2512           Chris Welch         Melody Maker  1971-05-22 00:00:00   \n",
       "s4315         David Hancock        Record Mirror  1974-12-14 00:00:00   \n",
       "s313        Geoffrey Cannon          unpublished  1965-07-01 00:00:00   \n",
       "s11701           Amy Linden  New York Daily News  1997-01-13 00:00:00   \n",
       "c1710         Adam Sweeting         The Guardian  1988-03-04 00:00:00   \n",
       "s5512       Davitt Sigerson          Black Music  1977-02-01 00:00:00   \n",
       "s12100      Marc Weingarten    Los Angeles Times  1999-04-19 00:00:00   \n",
       "s7118            Don Waller    Los Angeles Times  1982-08-10 00:00:00   \n",
       "c3603            Rob Hughes                Uncut  2011-08-01 00:00:00   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "s11916                                   [chaka-khan, prince]     soul   \n",
       "s2512                                           [buddy-miles]     soul   \n",
       "s4315                                                 [rufus]     soul   \n",
       "s313        [chuck-berry, ray-charles, billie-holiday, rol...     soul   \n",
       "s11701                [new-edition, blackstreet, keith-sweat]     soul   \n",
       "c1710                                           [lyle-lovett]  country   \n",
       "s5512                                               [boney-m]     soul   \n",
       "s12100                                [dru-hill, faith-evans]     soul   \n",
       "s7118                                [defunkt, talking-heads]     soul   \n",
       "c3603                                       [frank-fairfield]  country   \n",
       "\n",
       "                             type  \\\n",
       "article_id                          \n",
       "s11916                  Interview   \n",
       "s2512                 Live Review   \n",
       "s4315       Profile and Interview   \n",
       "s313                        Essay   \n",
       "s11701                Live Review   \n",
       "c1710       Profile and Interview   \n",
       "s5512                      Review   \n",
       "s12100                Live Review   \n",
       "s7118                 Live Review   \n",
       "c3603                      Review   \n",
       "\n",
       "                                                         href  length  \n",
       "article_id                                                             \n",
       "s11916      /Library/SearchLinkRedirect?folder=chaka-khan-...     820  \n",
       "s2512       /Library/SearchLinkRedirect?folder=buddy-miles...     305  \n",
       "s4315       /Library/SearchLinkRedirect?folder=going-below...     586  \n",
       "s313        /Library/SearchLinkRedirect?folder=from-pop-si...    2226  \n",
       "s11701      /Library/SearchLinkRedirect?folder=new-edition...     452  \n",
       "c1710       /Library/SearchLinkRedirect?folder=lyle-lovett...     903  \n",
       "s5512       /Library/SearchLinkRedirect?folder=boney-m-ita...     196  \n",
       "s12100      /Library/SearchLinkRedirect?folder=faith-evans...     218  \n",
       "s7118       /Library/SearchLinkRedirect?folder=talking-hea...     357  \n",
       "c3603       /Library/SearchLinkRedirect?folder=frank-fairf...     339  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c5a62",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing the Corpus<a name=\"paragraph3\"></a>\n",
    "I read in the body of each document and tokenize it with `nltk`'s `sent_tokenize()` and `WhitespaceTokenizer()` methods. Tokens are amassed in the `CORPUS` table and indexed by document OHCO structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7908aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for reading files\n",
    "def RBP_reader(filePath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(filePath, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, 'html')\n",
    "    \n",
    "    writer = soup.find(\"span\", class_=\"writer\") \\\n",
    "                 .get_text() \\\n",
    "                 .replace(r'\\n+', ' ') \\\n",
    "                 .strip()\n",
    "    \n",
    "    standfirst = soup.find(\"div\", class_=\"standfirst\").get_text()\n",
    "    copy = soup.find(\"div\", class_=\"copy\").get_text()\n",
    "\n",
    "    # if standfirst contains writer name, it is assumed that \n",
    "    # standfirst is purely metadata and should be ignored\n",
    "    if writer.lower() in standfirst.lower():\n",
    "        doc = copy\n",
    "    else:\n",
    "        doc = standfirst+copy\n",
    "    \n",
    "    return doc.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(library, filePrefix = \"./data/html/\"):\n",
    "    '''\n",
    "    Inputs\n",
    "    -----\n",
    "    library - pandas dataframe, must include author_id as index\n",
    "    fileprefix - string, path to directory containing .html files\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    CORPUS - pandas dataframe, contains all tokens in corpus\n",
    "    '''\n",
    "    import re\n",
    "    import nltk\n",
    "    \n",
    "    para_pat = r\"\\n{2,}\"\n",
    "    documents = []\n",
    "    \n",
    "    i=0\n",
    "    numIter = len(library)\n",
    "    \n",
    "    for id in library.index:\n",
    "        \n",
    "        doc = RBP_reader(filePrefix+str(id)+\".html\")\n",
    "        \n",
    "        PARAS = re.split(para_pat, doc)\n",
    "        PARAS = [x.strip().replace('\\n', ' ') for x in PARAS]\n",
    "        PARAS = pd.DataFrame(PARAS, columns = ['para_str'])\n",
    "        PARAS.index.name = 'para_id'\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame('sent_str')\n",
    "        \n",
    "        SENTS.index.names = ['para_id', 'sent_id']\n",
    "        \n",
    "        SENTS = SENTS.sent_str.str.replace(\"-\", \" \").to_frame()\n",
    "        SENTS = SENTS.sent_str.str.replace(\"/\", \" \").to_frame()\n",
    "        \n",
    "        TOKENS = SENTS.sent_str\\\n",
    "                      .apply(lambda x: \\\n",
    "                             pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))) \\\n",
    "                      .stack() \\\n",
    "                      .to_frame('pos_tuple')\n",
    "\n",
    "        TOKENS.index.names = OHCO[1:]\n",
    "        \n",
    "        TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "        TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "        TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "        \n",
    "        punc_pos = ['$', \"''\", '(', ')', '[', ']', ',', '--', '.', ':', '``']\n",
    "        TOKENS['term_str'] = TOKENS[~TOKENS.pos.isin(punc_pos)].token_str \\\n",
    "                                .str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS['article_id'] = id\n",
    "        TOKENS = TOKENS.reset_index().set_index(OHCO)\n",
    "        \n",
    "        documents.append(TOKENS)\n",
    "        \n",
    "        if i % round(numIter/5) == 0:\n",
    "            print(f\"{round(i*100/numIter)}% Complete\")\n",
    "        i +=1\n",
    "\n",
    "    # sort index & columns\n",
    "    CORPUS = pd.concat(documents).sort_index()\n",
    "    CORPUS = CORPUS[['token_str', 'term_str', 'pos_tuple', 'pos']]\n",
    "    \n",
    "    # add POS_group\n",
    "    CORPUS['pos_group'] = CORPUS.pos.str.slice(0,2)\n",
    "    \n",
    "    del(documents)\n",
    "    del(PARAS)\n",
    "    del(SENTS)\n",
    "    del(TOKENS)\n",
    "    \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be5dfb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% Complete\n",
      "20% Complete\n",
      "40% Complete\n",
      "60% Complete\n",
      "80% Complete\n",
      "100% Complete\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac35542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank tokens and NANs\n",
    "CORPUS = CORPUS[CORPUS.term_str!='']\n",
    "CORPUS = CORPUS[~CORPUS.term_str.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f5612",
   "metadata": {},
   "source": [
    "**Adding article length to `LIB`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81a170a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['length'] = CORPUS.groupby('article_id').term_str.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833cd37",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting a Vocabulary<a name=\"paragraph4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6b1fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VOCAB table\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n')\n",
    "VOCAB.index.name = 'term_str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba38c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max part-of-speech\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "\n",
    "# add max POS group\n",
    "VOCAB['max_pos_group'] = VOCAB.max_pos.str.slice(0,2)\n",
    "\n",
    "# add number of POS associated with each term\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "996a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add term-based statistics\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "732578ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>df</th>\n",
       "      <th>tfidf_mean</th>\n",
       "      <th>tfidf_max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>brittens</th>\n",
       "      <td>2</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NNP}</td>\n",
       "      <td>8</td>\n",
       "      <td>5.158690e-07</td>\n",
       "      <td>20.886492</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.018073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1in</th>\n",
       "      <td>2</td>\n",
       "      <td>CD</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>{CD}</td>\n",
       "      <td>3</td>\n",
       "      <td>5.158690e-07</td>\n",
       "      <td>20.886492</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.078045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blank</th>\n",
       "      <td>46</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>8</td>\n",
       "      <td>{NNP, VBZ, VBD, RB, VBP, IN, JJ, NN}</td>\n",
       "      <td>5</td>\n",
       "      <td>1.186499e-05</td>\n",
       "      <td>16.362930</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.096152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinning</th>\n",
       "      <td>3</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VBG}</td>\n",
       "      <td>7</td>\n",
       "      <td>7.738036e-07</td>\n",
       "      <td>20.301529</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.033173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supporters</th>\n",
       "      <td>17</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>{PDT, NN, NNS}</td>\n",
       "      <td>10</td>\n",
       "      <td>4.384887e-06</td>\n",
       "      <td>17.799029</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.072469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n max_pos max_pos_group  n_pos  \\\n",
       "term_str                                      \n",
       "brittens     2     NNP            NN      1   \n",
       "1in          2      CD            CD      1   \n",
       "blank       46      NN            NN      8   \n",
       "pinning      3     VBG            VB      1   \n",
       "supporters  17     NNS            NN      3   \n",
       "\n",
       "                                         cat_pos  n_chars             p  \\\n",
       "term_str                                                                  \n",
       "brittens                                   {NNP}        8  5.158690e-07   \n",
       "1in                                         {CD}        3  5.158690e-07   \n",
       "blank       {NNP, VBZ, VBD, RB, VBP, IN, JJ, NN}        5  1.186499e-05   \n",
       "pinning                                    {VBG}        7  7.738036e-07   \n",
       "supporters                        {PDT, NN, NNS}       10  4.384887e-06   \n",
       "\n",
       "                    i  df  tfidf_mean  tfidf_max  \n",
       "term_str                                          \n",
       "brittens    20.886492   1    0.000005   0.018073  \n",
       "1in         20.886492   2    0.000035   0.078045  \n",
       "blank       16.362930  42    0.000387   0.096152  \n",
       "pinning     20.301529   3    0.000017   0.033173  \n",
       "supporters  17.799029  17    0.000178   0.072469  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f0c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a Bag-of-Words<a name=\"paragraph5\"></a>\n",
    "I create a bag-of-words model for each document in the corpus. In this representation, the frequency of each word is recorded while grammar and word order is discarded. Rather than record simple word counts, I use **tf-idf** which weights words based on importance. I use scikit-learn's `CountVectorizer` and `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba7874c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather CORPUS into articles\n",
    "ARTICLES = CORPUS.groupby('article_id').term_str.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffcc9c",
   "metadata": {},
   "source": [
    "I create 2 bags-of-words. The first contains the top 32,000 unigrams with stop words removed. The limit of 32,000 was chosen because it is roughly half the number of terms in the vocabulary. Additionally, the top 32,000 terms occur at least 3 times in the corpus. \n",
    "\n",
    "The second bag-of-words contains the top 10000 N-grams where $\\{N: 1,2,3,4\\}$. Again, stop words are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28921fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBOW(documents, ngram_range = (1,1), max_features = None):\n",
    "    \n",
    "    # initialize CountVectorizer\n",
    "    count_engine = CountVectorizer(stop_words = 'english',\n",
    "                                   ngram_range = ngram_range,\n",
    "                                   max_features = max_features)\n",
    "    \n",
    "    # fit/transform\n",
    "    X1 = count_engine.fit_transform(documents)\n",
    "    \n",
    "    # Create Document-Term matrix from output\n",
    "    DTM = pd.DataFrame(X1.toarray(),\n",
    "                       columns = count_engine.get_feature_names_out(),\n",
    "                       index = documents.index)\n",
    "    \n",
    "    # initialize TfidfTransformer\n",
    "    tfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n",
    "    \n",
    "    # fit/transofrm\n",
    "    X2 = tfidf_engine.fit_transform(DTM)\n",
    "    \n",
    "    # create TFIDF table\n",
    "    TFIDF = pd.DataFrame(X2.toarray(),\n",
    "                         columns = DTM.columns,\n",
    "                         index=DTM.index)\n",
    "    \n",
    "    # create BOW\n",
    "    BOW = DTM[DTM > 0].stack().to_frame('n') \\\n",
    "        .join(TFIDF[TFIDF > 0].stack().to_frame('tfidf'))\n",
    "    BOW.index.rename('term_str', level = 1, inplace = True)\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "755b7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigram BOW\n",
    "BOW_unigrams = createBOW(ARTICLES, ngram_range = (1,1), max_features = 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dd61580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Ngram BOW\n",
    "BOW_ngrams = createBOW(ARTICLES, ngram_range = (1,4), max_features = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c1def",
   "metadata": {},
   "source": [
    "#### Add Frequency Features to `VOCAB`<a name=\"subparagraph3\"></a>\n",
    "I add the following features to the `VOCAB` table.\n",
    "+ `tfidf_mean` - Average tfidf for all occurences in corpus\n",
    "+ `tfidf_max` - Max tfidf value of term\n",
    "+ `df` - Document Frequency, count of documents in which the term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8f6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountVectorizer with default params, but custom tokenizer\n",
    "vocab_engine = CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "\n",
    "# fit/transform\n",
    "vocab_counts = vocab_engine.fit_transform(ARTICLES)\n",
    "\n",
    "# convert to dataframe\n",
    "DOC_TERM = pd.DataFrame(vocab_counts.toarray(),\n",
    "                        columns = vocab_engine.get_feature_names_out(),\n",
    "                       index = ARTICLES.index)\n",
    "# append to VOCAB\n",
    "VOCAB['df'] = DOC_TERM[DOC_TERM > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "806f6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TfidfTransformer\n",
    "vocab_tfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n",
    "\n",
    "# fit/transform\n",
    "vocab_tfidf = vocab_tfidf_engine.fit_transform(DOC_TERM)\n",
    "\n",
    "# create TFIDF dataframe\n",
    "TFIDF = pd.DataFrame(vocab_tfidf.toarray(),\n",
    "                    columns = DOC_TERM.columns,\n",
    "                    index =  DOC_TERM.index)\n",
    "# add to VOCAB\n",
    "VOCAB['tfidf_mean'] = TFIDF.mean()\n",
    "VOCAB['tfidf_max'] = TFIDF.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677ccd5",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving the Digital Analytical Edition<a name=\"paragraph6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4ee14",
   "metadata": {},
   "source": [
    "With the creation of the `LIB`, `CORPUS`, `VOCAB`, and `BOW` tables, I have established a foundation on which I can begin my analysis. I save these tables to separate `.csv` files for ease of use. In the following notebook, I will use each of these tables to perform principal component analysis, topic modelling, word embeddings, and sentiment analysis. Along the way, I will continue to build out the digital analytical edition by adding features to each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03af3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "LIB.to_csv('./data/LIB.csv')\n",
    "CORPUS.to_csv('./data/CORPUS.csv')\n",
    "VOCAB.to_csv('./data/VOCAB.csv')\n",
    "BOW_unigrams.to_csv('./data/BOW_unigrams.csv')\n",
    "BOW_ngrams.to_csv('./data/BOW_ngrams.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
