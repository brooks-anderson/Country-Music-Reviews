{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774c204d",
   "metadata": {},
   "source": [
    "# Country & Soul\n",
    "### *Analyzing Trends in American Music Journalism from 1960 to Present*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19f2c5",
   "metadata": {},
   "source": [
    "# Digital Analytical Edition\n",
    "#### Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries & Set Up](#paragraph1)\n",
    "3. [Creating the LIB table](#paragraph2)\n",
    "    1. [Parsing Dates](#subparagraph1)\n",
    "    2. [Narrowing the Scope](#subparagraph2)\n",
    "4. [Constructing the Corpus](#paragraph3)\n",
    "5. [Extracting a Vocabulary](#paragraph4)\n",
    "6. [Creating a Bag-of-Words](#paragraph5)\n",
    "    1. [Add Frequency Features to `VOCAB`](#subparagraph3)\n",
    "7. [Saving the Digital Analytic Edition](#paragraph6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6330874",
   "metadata": {},
   "source": [
    "## Introduction<a name=\"introduction\"></a>\n",
    "In this notebook, I aggregate and parse the source files to create a Digital Analytical Edition (DAE).  \n",
    "\n",
    "The DAE contains the following tables:  \n",
    "+ `LIB.csv`    - Metadata for each article.\n",
    "+ `CORPUS.csv` - Aggregated text from source files. Indexed by Ordered Hierarchy of Content Object (OHCO) structure.\n",
    "+ `VOCAB.csv`  - Linguistic features and statistics for words in `CORPUS.csv`. Stop words removed.\n",
    "+ `BOW.csv`    - Bag-of-words representation of `CORPUS.csv` with stop words removed.\n",
    "\n",
    "Source files were scraped from [rocksbackpages.com](https://www.rocksbackpages.com/) using `rbpscraper.py`. I present an example of `rbpscraper.py` functionality here. It is **not** recommended that the reader attempts to run this code. `rbpscraper.py` requires a subscription to rocksbackpages.com and several hours of your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3feb3be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste URL:\n",
      "https://www.rocksbackpages.com/\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Please paste cookies:\n",
      "sample=cookies; auth=details\n",
      "\n",
      "Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from rbpscraper import RBPScraper\n",
    "\n",
    "country = RBPScraper(desc = \"country\", write_path = \"./datadir/\")\n",
    "    # desc - description of articles to be scraped\n",
    "    # write_path - path to directory where articles will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ef282",
   "metadata": {},
   "source": [
    "After instantiating an RBPScraper object, the user will be prompted to enter the url for the first page of search results. Then, the user will be prompted to enter cookies for authentification. The web-scraping process can be initiated by calling the following methods. This will save articles as html files and a metadata table as a csv file to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd079578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country.searchScraper().articleScraper().writeLIB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cbd2d",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries & Set Up<a name=\"paragraph1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa0e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2eedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "dataPath = \"./data/\"\n",
    "articlePath = \"./data/html/\"\n",
    "\n",
    "# define OHCO structure\n",
    "OHCO = ['article_id', 'para_id', 'sent_id', 'token_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484f61b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating the Library Table<a name=\"paragraph2\"></a>\n",
    "Two library tables were created during the webscraping process. I combine, process, and refine the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00eee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in metadata table for each genre\n",
    "country_metadata = pd.read_csv(f\"{dataPath}cLIB.csv\")\n",
    "country_metadata.set_index(\"id\", inplace = True)\n",
    "\n",
    "soul_metadata = pd.read_csv(f\"{dataPath}sLIB.csv\")\n",
    "soul_metadata.set_index(\"id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e86e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LIB table\n",
    "LIB = pd.concat([country_metadata, soul_metadata])\n",
    "\n",
    "LIB.index.rename(OHCO[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3b231",
   "metadata": {},
   "source": [
    "#### Parsing Dates<a name=\"subparagraph1\"></a>\n",
    "Inspection reveals that the publication dates are not in a standard format. I define a function that parses date strings. The following table describes how dates are assigned to ambiguous formats.\n",
    "\n",
    "|`date` format|output format|\n",
    "|------------|-------------|\n",
    "|Month *year*|YYYY-mm-01|\n",
    "|*year*|YYYY-07-01|\n",
    "|Summer *year*|YYYY-08-01|\n",
    "|Fall *year*|YYYY-11-01|\n",
    "|Winter *year*|YYYY-02-01|\n",
    "|Spring *year*|YYYY-05-01|\n",
    "\n",
    "Any date string that is missing or does not adhere to one of the above formats will be corrected by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_date(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%d %B %Y\")\n",
    "\n",
    "def year_only(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dt_obj = datetime.strptime(dt, \"%Y\").date()\n",
    "    return dt_obj.replace(month = 7, day = 1)\n",
    "\n",
    "def month_year(dt):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    return datetime.strptime(dt, \"%B %Y\").date()\n",
    "\n",
    "def season_year(dt):\n",
    "    from datetime import datetime, date\n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        season = dt.split()[-2].lower()\n",
    "        year = dt.split()[-1]\n",
    "\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "    \n",
    "    if season == 'spring':\n",
    "        return date(int(year), 5, 1)\n",
    "    elif season == 'summer':\n",
    "        return date(int(year), 8, 1)\n",
    "    elif season == 'fall':\n",
    "        return date(int(year), 11, 1)\n",
    "    elif season == 'winter':\n",
    "        return date(int(year), 2, 1)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def date_parser(dt):\n",
    "    '''\n",
    "    Converts publication date strings from rocksbackpages.com articles\n",
    "    to date objects with format YYYY-mm-dd.\n",
    "    '''\n",
    "    helpers = [normal_date, year_only, month_year, season_year]\n",
    "    \n",
    "    for f in helpers:\n",
    "        try:\n",
    "            return f(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5e5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['date_parsed'] = LIB.date.apply(date_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4a1692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>date_parsed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c610</th>\n",
       "      <td>Charley Pride, Tammy Wynette and George Jones:...</td>\n",
       "      <td>Gene Guerrero</td>\n",
       "      <td>The Great Speckled Bird</td>\n",
       "      <td>31 February 1972</td>\n",
       "      <td>['george-jones', 'tammy-wynette', 'charley-pri...</td>\n",
       "      <td>country</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=charley-pri...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12801</th>\n",
       "      <td>Duffy: Rockferry</td>\n",
       "      <td>Gavin Martin</td>\n",
       "      <td>Daily Mirror</td>\n",
       "      <td>29 February 2002</td>\n",
       "      <td>['duffy']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=duffy-irock...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12916</th>\n",
       "      <td>Earth Wind And Fire: Beacon Theater, New York</td>\n",
       "      <td>Kandia Crazy Horse</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>15 2003</td>\n",
       "      <td>['earth-wind--fire']</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=earth-wind-...</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "c610        Charley Pride, Tammy Wynette and George Jones:...   \n",
       "s12801                                       Duffy: Rockferry   \n",
       "s12916          Earth Wind And Fire: Beacon Theater, New York   \n",
       "\n",
       "                        author                   source              date  \\\n",
       "article_id                                                                  \n",
       "c610             Gene Guerrero  The Great Speckled Bird  31 February 1972   \n",
       "s12801            Gavin Martin             Daily Mirror  29 February 2002   \n",
       "s12916      Kandia Crazy Horse               PopMatters           15 2003   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "c610        ['george-jones', 'tammy-wynette', 'charley-pri...  country   \n",
       "s12801                                              ['duffy']     soul   \n",
       "s12916                                   ['earth-wind--fire']     soul   \n",
       "\n",
       "                   type                                               href  \\\n",
       "article_id                                                                   \n",
       "c610        Live Review  /Library/SearchLinkRedirect?folder=charley-pri...   \n",
       "s12801           Review  /Library/SearchLinkRedirect?folder=duffy-irock...   \n",
       "s12916      Live Review  /Library/SearchLinkRedirect?folder=earth-wind-...   \n",
       "\n",
       "           date_parsed  \n",
       "article_id              \n",
       "c610               NaT  \n",
       "s12801             NaT  \n",
       "s12916             NaT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.loc[LIB.date_parsed.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0faab7",
   "metadata": {},
   "source": [
    "Only three dates failed to parse. I will manually correct them. After that, I will replace the `date` column with `date_parsed` as keeping both is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4121cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.loc['c610', 'date_parsed'] = date(1972, 2, 29)\n",
    "LIB.loc[\"s12801\", 'date_parsed'] = date(2002, 2, 28)\n",
    "LIB.loc[\"s12916\", 'date_parsed'] = date(2003, 7, 1)\n",
    "\n",
    "# replace date col\n",
    "LIB['date'] = LIB.date_parsed\n",
    "LIB.drop(columns ='date_parsed', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d185",
   "metadata": {},
   "source": [
    "Next, I notice that the `subjects` column is a string representation of a list--not an actual list. `subjects` is converted to list type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37dfe556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "LIB.subjects = LIB.subjects.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa665f",
   "metadata": {},
   "source": [
    "#### Narrowing the Scope<a name=\"subparagraph2\"></a>\n",
    "I refine the Focus of my analysis rather than work with all 4000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5861589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 articles missing a subject were dropped\n"
     ]
    }
   ],
   "source": [
    "# drop articles not tagged with a subject\n",
    "empty_subjects = [True if x == [] else False for x in LIB.subjects ]\n",
    "print(f\"{sum(empty_subjects)} articles missing a subject were dropped\")\n",
    "\n",
    "LIB = LIB[[not x for x in empty_subjects]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6f89f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interview</th>\n",
       "      <th>Review</th>\n",
       "      <th>Live Review</th>\n",
       "      <th>Profile and Interview</th>\n",
       "      <th>Report and Interview</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Report</th>\n",
       "      <th>Book Excerpt</th>\n",
       "      <th>Obituary</th>\n",
       "      <th>Retrospective</th>\n",
       "      <th>...</th>\n",
       "      <th>Review and Interview</th>\n",
       "      <th>Film/DVD/TV Review</th>\n",
       "      <th>Guide</th>\n",
       "      <th>Audio transcript of interview</th>\n",
       "      <th>Discography</th>\n",
       "      <th>Special Feature</th>\n",
       "      <th>Readers' Letters</th>\n",
       "      <th>Column</th>\n",
       "      <th>Letters</th>\n",
       "      <th>Film/DVD Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <td>1016</td>\n",
       "      <td>905</td>\n",
       "      <td>762</td>\n",
       "      <td>301</td>\n",
       "      <td>155</td>\n",
       "      <td>109</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>101</td>\n",
       "      <td>88</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Interview  Review  Live Review  Profile and Interview  \\\n",
       "type       1016     905          762                    301   \n",
       "\n",
       "      Report and Interview  Profile  Report  Book Excerpt  Obituary  \\\n",
       "type                   155      109     106           104       101   \n",
       "\n",
       "      Retrospective  ...  Review and Interview  Film/DVD/TV Review  Guide  \\\n",
       "type             88  ...                    18                  16      6   \n",
       "\n",
       "      Audio transcript of interview  Discography  Special Feature  \\\n",
       "type                              5            5                4   \n",
       "\n",
       "      Readers' Letters  Column  Letters  Film/DVD Review  \n",
       "type                 3       1        1                1  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refine article types\n",
    "\n",
    "# consolidate 'Sleeve and programme notes' with 'Sleevenotes'\n",
    "LIB.loc[LIB.type == 'Sleeve and programme notes', 'type'] = 'Sleevenotes'\n",
    "\n",
    "LIB.type.value_counts().to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd0ba5",
   "metadata": {},
   "source": [
    "Immediately, I see some document types that should be excluded: *Film/DVD Review*, *Letters*, *Readers' Letters*. These documents do not fit with the broader corpus as they are either not about music, or not written by professional journalists.  \n",
    "\n",
    "It is debatable whether *obituary* and *memoir* should be included. These document types are dissimilar from other documents in the corpus as they contain more biographical language. However, if an artist has an associated *obituary* or *memoir*, they were likely to have had significant cultural impact. This analysis is conducted at the level of genre rather than artist. Thus, I discard *obituary* and *memoir* in favor of reducing the size of the corpus.\n",
    "\n",
    "Lastly, I reason that *book excerpts* should also be removed. Out of the 104 book excerpts, 97 are sourced from a single book, *The Faber Companion to 20<sup>th</sup> Century Popular Music*. These excerpts suffer the same issues of unsuitability as the obituaries; they are more pragmatic than they are poetic. Additionally, including so many documents from a single source would almost certainly have an unintended effect on the analysis.\n",
    "\n",
    "The remaining documents discuss musicians, alblums, and performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ec12b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 3651\n"
     ]
    }
   ],
   "source": [
    "exclude = ['Film/DVD Review', 'Film/DVD/TV Review',\n",
    "           'Letters', \"Readers' Letters\", \n",
    "           'Obituary', 'Memoir', \n",
    "           'Book Excerpt', 'Book Review',\n",
    "           'Audio transcript of interview']\n",
    "\n",
    "LIB = LIB.loc[~LIB.type.isin(exclude)]\n",
    "print(f\"Number of Articles: {len(LIB)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79faf4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>subjects</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>href</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>s15507</th>\n",
       "      <td>Grace Jones et al.: Love Supreme Jazz Festival...</td>\n",
       "      <td>Nick Hasted</td>\n",
       "      <td>The Independent</td>\n",
       "      <td>2016-07-07 00:00:00</td>\n",
       "      <td>[burt-bacharach, grace-jones, kamasi-washingto...</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=grace-jones...</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3802</th>\n",
       "      <td>Kris Kristofferson</td>\n",
       "      <td>David Burke</td>\n",
       "      <td>R2/Rock'n'Reel</td>\n",
       "      <td>2013-09-01 00:00:00</td>\n",
       "      <td>[kris-kristofferson]</td>\n",
       "      <td>country</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=kris-kristo...</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s11106</th>\n",
       "      <td>Rick James: The Untold Story</td>\n",
       "      <td>Michael Goldberg</td>\n",
       "      <td>Vibe</td>\n",
       "      <td>1994-04-01 00:00:00</td>\n",
       "      <td>[rick-james]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=rick-james-...</td>\n",
       "      <td>3594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s5018</th>\n",
       "      <td>Chuck Jackson: Chuck's Foot Is Back On The Thr...</td>\n",
       "      <td>Kevin Allen</td>\n",
       "      <td>Record Mirror</td>\n",
       "      <td>1975-12-06 00:00:00</td>\n",
       "      <td>[chuck-jackson]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Report and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=chuck-jacks...</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c903</th>\n",
       "      <td>Scott Walker: We Had It All</td>\n",
       "      <td>Fred Dellar</td>\n",
       "      <td>New Musical Express</td>\n",
       "      <td>1974-09-21 00:00:00</td>\n",
       "      <td>[scott-walker]</td>\n",
       "      <td>country</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=scott-walke...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3816</th>\n",
       "      <td>Country music's gay stars: \"We're still kickin...</td>\n",
       "      <td>Graeme Thomson</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>2014-04-10 00:00:00</td>\n",
       "      <td>[chely-wright, lavender-country]</td>\n",
       "      <td>country</td>\n",
       "      <td>Report and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=country-mus...</td>\n",
       "      <td>1403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s12606</th>\n",
       "      <td>Seeking Tuneage Kicks With... Kelis</td>\n",
       "      <td>Dorian Lynskey</td>\n",
       "      <td>Select</td>\n",
       "      <td>2001-01-01 00:00:00</td>\n",
       "      <td>[kelis]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=seeking-tun...</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s15410</th>\n",
       "      <td>Khruangbin: The Universe Smiles Upon You</td>\n",
       "      <td>Daryl Easlea</td>\n",
       "      <td>MOJO</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>[khruangbin]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=khruangbin-...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s7113</th>\n",
       "      <td>Darlene Hits Local Scene, Reviving an Old Spector</td>\n",
       "      <td>Joel Selvin</td>\n",
       "      <td>San Francisco Chronicle</td>\n",
       "      <td>1982-07-18 00:00:00</td>\n",
       "      <td>[darlene-love, phil-spector]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Profile and Interview</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=darlene-hit...</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s1300</th>\n",
       "      <td>The Righteous Brothers: Greek Theatre, Los Ang...</td>\n",
       "      <td>June Harris</td>\n",
       "      <td>New Musical Express</td>\n",
       "      <td>1967-09-23 00:00:00</td>\n",
       "      <td>[righteous-brothers-the]</td>\n",
       "      <td>soul</td>\n",
       "      <td>Live Review</td>\n",
       "      <td>/Library/SearchLinkRedirect?folder=the-righteo...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "article_id                                                      \n",
       "s15507      Grace Jones et al.: Love Supreme Jazz Festival...   \n",
       "c3802                                      Kris Kristofferson   \n",
       "s11106                           Rick James: The Untold Story   \n",
       "s5018       Chuck Jackson: Chuck's Foot Is Back On The Thr...   \n",
       "c903                              Scott Walker: We Had It All   \n",
       "c3816       Country music's gay stars: \"We're still kickin...   \n",
       "s12606                    Seeking Tuneage Kicks With... Kelis   \n",
       "s15410               Khruangbin: The Universe Smiles Upon You   \n",
       "s7113       Darlene Hits Local Scene, Reviving an Old Spector   \n",
       "s1300       The Righteous Brothers: Greek Theatre, Los Ang...   \n",
       "\n",
       "                      author                   source                 date  \\\n",
       "article_id                                                                   \n",
       "s15507           Nick Hasted          The Independent  2016-07-07 00:00:00   \n",
       "c3802            David Burke           R2/Rock'n'Reel  2013-09-01 00:00:00   \n",
       "s11106      Michael Goldberg                     Vibe  1994-04-01 00:00:00   \n",
       "s5018            Kevin Allen            Record Mirror  1975-12-06 00:00:00   \n",
       "c903             Fred Dellar      New Musical Express  1974-09-21 00:00:00   \n",
       "c3816         Graeme Thomson             The Guardian  2014-04-10 00:00:00   \n",
       "s12606        Dorian Lynskey                   Select  2001-01-01 00:00:00   \n",
       "s15410          Daryl Easlea                     MOJO  2016-01-01 00:00:00   \n",
       "s7113            Joel Selvin  San Francisco Chronicle  1982-07-18 00:00:00   \n",
       "s1300            June Harris      New Musical Express  1967-09-23 00:00:00   \n",
       "\n",
       "                                                     subjects    topic  \\\n",
       "article_id                                                               \n",
       "s15507      [burt-bacharach, grace-jones, kamasi-washingto...     soul   \n",
       "c3802                                    [kris-kristofferson]  country   \n",
       "s11106                                           [rick-james]     soul   \n",
       "s5018                                         [chuck-jackson]     soul   \n",
       "c903                                           [scott-walker]  country   \n",
       "c3816                        [chely-wright, lavender-country]  country   \n",
       "s12606                                                [kelis]     soul   \n",
       "s15410                                           [khruangbin]     soul   \n",
       "s7113                            [darlene-love, phil-spector]     soul   \n",
       "s1300                                [righteous-brothers-the]     soul   \n",
       "\n",
       "                             type  \\\n",
       "article_id                          \n",
       "s15507                Live Review   \n",
       "c3802                   Interview   \n",
       "s11106                  Interview   \n",
       "s5018        Report and Interview   \n",
       "c903                       Review   \n",
       "c3816        Report and Interview   \n",
       "s12606                  Interview   \n",
       "s15410                     Review   \n",
       "s7113       Profile and Interview   \n",
       "s1300                 Live Review   \n",
       "\n",
       "                                                         href  length  \n",
       "article_id                                                             \n",
       "s15507      /Library/SearchLinkRedirect?folder=grace-jones...     430  \n",
       "c3802       /Library/SearchLinkRedirect?folder=kris-kristo...    1939  \n",
       "s11106      /Library/SearchLinkRedirect?folder=rick-james-...    3594  \n",
       "s5018       /Library/SearchLinkRedirect?folder=chuck-jacks...     881  \n",
       "c903        /Library/SearchLinkRedirect?folder=scott-walke...     101  \n",
       "c3816       /Library/SearchLinkRedirect?folder=country-mus...    1403  \n",
       "s12606      /Library/SearchLinkRedirect?folder=seeking-tun...    1106  \n",
       "s15410      /Library/SearchLinkRedirect?folder=khruangbin-...     149  \n",
       "s7113       /Library/SearchLinkRedirect?folder=darlene-hit...     541  \n",
       "s1300       /Library/SearchLinkRedirect?folder=the-righteo...     129  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c5a62",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing the Corpus<a name=\"paragraph3\"></a>\n",
    "I read in the body of each document and tokenize it with `nltk`'s `sent_tokenize()` and `WhitespaceTokenizer()` methods. Tokens are amassed in the `CORPUS` table and indexed by document OHCO structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7908aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for reading files\n",
    "def RBP_reader(filePath):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    with open(filePath, 'r', encoding = 'utf-8') as f:\n",
    "        contents = f.read()\n",
    "        \n",
    "    soup = BeautifulSoup(contents, 'html')\n",
    "    \n",
    "    writer = soup.find(\"span\", class_=\"writer\") \\\n",
    "                 .get_text() \\\n",
    "                 .replace(r'\\n+', ' ') \\\n",
    "                 .strip()\n",
    "    \n",
    "    standfirst = soup.find(\"div\", class_=\"standfirst\").get_text()\n",
    "    copy = soup.find(\"div\", class_=\"copy\").get_text()\n",
    "\n",
    "    # if standfirst contains writer name, it is assumed that \n",
    "    # standfirst is purely metadata and should be ignored\n",
    "    if writer.lower() in standfirst.lower():\n",
    "        doc = copy\n",
    "    else:\n",
    "        doc = standfirst+copy\n",
    "    \n",
    "    return doc.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(library, filePrefix = \"./data/html/\"):\n",
    "    '''\n",
    "    Inputs\n",
    "    -----\n",
    "    library - pandas dataframe, must include author_id as index\n",
    "    fileprefix - string, path to directory containing .html files\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    CORPUS - pandas dataframe, contains all tokens in corpus\n",
    "    '''\n",
    "    import re\n",
    "    import nltk\n",
    "    \n",
    "    para_pat = r\"\\n{2,}\"\n",
    "    documents = []\n",
    "    \n",
    "    i=0\n",
    "    numIter = len(library)\n",
    "    \n",
    "    for id in library.index:\n",
    "        \n",
    "        doc = RBP_reader(filePrefix+str(id)+\".html\")\n",
    "        \n",
    "        PARAS = re.split(para_pat, doc)\n",
    "        PARAS = [x.strip().replace('\\n', ' ') for x in PARAS]\n",
    "        PARAS = pd.DataFrame(PARAS, columns = ['para_str'])\n",
    "        PARAS.index.name = 'para_id'\n",
    "        \n",
    "        SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame('sent_str')\n",
    "        \n",
    "        SENTS.index.names = ['para_id', 'sent_id']\n",
    "        \n",
    "        SENTS = SENTS.sent_str.str.replace(\"-\", \" \").to_frame()\n",
    "        SENTS = SENTS.sent_str.str.replace(\"/\", \" \").to_frame()\n",
    "        \n",
    "        TOKENS = SENTS.sent_str\\\n",
    "                      .apply(lambda x: \\\n",
    "                             pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x)))) \\\n",
    "                      .stack() \\\n",
    "                      .to_frame('pos_tuple')\n",
    "\n",
    "        TOKENS.index.names = OHCO[1:]\n",
    "        \n",
    "        TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])\n",
    "        TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])\n",
    "        TOKENS['term_str'] = TOKENS.token_str.str.lower()\n",
    "        \n",
    "        punc_pos = ['$', \"''\", '(', ')', '[', ']', ',', '--', '.', ':', '``']\n",
    "        TOKENS['term_str'] = TOKENS[~TOKENS.pos.isin(punc_pos)].token_str \\\n",
    "                                .str.replace(r'[\\W_]+', '', regex=True).str.lower()\n",
    "\n",
    "        TOKENS['article_id'] = id\n",
    "        TOKENS = TOKENS.reset_index().set_index(OHCO)\n",
    "        \n",
    "        documents.append(TOKENS)\n",
    "        \n",
    "        if i % round(numIter/5) == 0:\n",
    "            print(f\"{round(i*100/numIter)}% Complete\")\n",
    "        i +=1\n",
    "\n",
    "    # sort index & columns\n",
    "    CORPUS = pd.concat(documents).sort_index()\n",
    "    CORPUS = CORPUS[['token_str', 'term_str', 'pos_tuple', 'pos']]\n",
    "    \n",
    "    # add POS_group\n",
    "    CORPUS['pos_group'] = CORPUS.pos.str.slice(0,2)\n",
    "    \n",
    "    del(documents)\n",
    "    del(PARAS)\n",
    "    del(SENTS)\n",
    "    del(TOKENS)\n",
    "    \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be5dfb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% Complete\n",
      "20% Complete\n",
      "40% Complete\n",
      "60% Complete\n",
      "80% Complete\n",
      "100% Complete\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac35542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove blank tokens and NANs\n",
    "CORPUS = CORPUS[CORPUS.term_str!='']\n",
    "CORPUS = CORPUS[~CORPUS.term_str.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f5612",
   "metadata": {},
   "source": [
    "**Adding article length to `LIB`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81a170a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['length'] = CORPUS.groupby('article_id').term_str.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833cd37",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting a Vocabulary<a name=\"paragraph4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6b1fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create VOCAB table\n",
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba38c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max part-of-speech\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "\n",
    "# add max POS group\n",
    "VOCAB['max_pos_group'] = VOCAB.max_pos.str.slice(0,2)\n",
    "\n",
    "# add number of POS associated with each term\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "996a3aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add term-based statistics\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "732578ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deena</th>\n",
       "      <td>2</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NNP}</td>\n",
       "      <td>5</td>\n",
       "      <td>5.158690e-07</td>\n",
       "      <td>20.886492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catsuit</th>\n",
       "      <td>2</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "      <td>{NN}</td>\n",
       "      <td>7</td>\n",
       "      <td>5.158690e-07</td>\n",
       "      <td>20.886492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastored</th>\n",
       "      <td>1</td>\n",
       "      <td>VBN</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "      <td>{VBN}</td>\n",
       "      <td>8</td>\n",
       "      <td>2.579345e-07</td>\n",
       "      <td>21.886492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pines</th>\n",
       "      <td>19</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NN</td>\n",
       "      <td>4</td>\n",
       "      <td>{NNP, NNPS, NN, NNS}</td>\n",
       "      <td>5</td>\n",
       "      <td>4.900756e-06</td>\n",
       "      <td>17.638564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enlightenment</th>\n",
       "      <td>6</td>\n",
       "      <td>NN</td>\n",
       "      <td>NN</td>\n",
       "      <td>3</td>\n",
       "      <td>{JJ, NN, NNS}</td>\n",
       "      <td>13</td>\n",
       "      <td>1.547607e-06</td>\n",
       "      <td>19.301529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n max_pos max_pos_group  n_pos               cat_pos  n_chars  \\\n",
       "deena           2     NNP            NN      1                 {NNP}        5   \n",
       "catsuit         2      NN            NN      1                  {NN}        7   \n",
       "pastored        1     VBN            VB      1                 {VBN}        8   \n",
       "pines          19     NNP            NN      4  {NNP, NNPS, NN, NNS}        5   \n",
       "enlightenment   6      NN            NN      3         {JJ, NN, NNS}       13   \n",
       "\n",
       "                          p          i  \n",
       "deena          5.158690e-07  20.886492  \n",
       "catsuit        5.158690e-07  20.886492  \n",
       "pastored       2.579345e-07  21.886492  \n",
       "pines          4.900756e-06  17.638564  \n",
       "enlightenment  1.547607e-06  19.301529  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f0c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a Bag-of-Words<a name=\"paragraph5\"></a>\n",
    "I create a bag-of-words model for each document in the corpus. In this representation, the frequency of each word is recorded while grammar and word order is discarded. Rather than record simple word counts, I use **tf-idf** which weights words based on importance. I use scikit-learn's `CountVectorizer` and `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba7874c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather CORPUS into articles\n",
    "ARTICLES = CORPUS.groupby('article_id').term_str.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcffcc9c",
   "metadata": {},
   "source": [
    "I create 2 bags-of-words. The first contains the top 32,000 unigrams with stop words removed. The limit of 32,000 was chosen because it is roughly half the number of terms in the vocabulary. Additionally, the top 32,000 terms occur at least 3 times in the corpus. \n",
    "\n",
    "The second bag-of-words contains the top 10000 N-grams where $\\{N: 1,2,3,4\\}$. Again, stop words are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28921fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBOW(documents, ngram_range = (1,1), max_features = None):\n",
    "    \n",
    "    # initialize CountVectorizer\n",
    "    count_engine = CountVectorizer(stop_words = 'english',\n",
    "                                   ngram_range = ngram_range,\n",
    "                                   max_features = max_features)\n",
    "    \n",
    "    # fit/transform\n",
    "    X1 = count_engine.fit_transform(documents)\n",
    "    \n",
    "    # Create Document-Term matrix from output\n",
    "    DTM = pd.DataFrame(X1.toarray(),\n",
    "                       columns = count_engine.get_feature_names_out(),\n",
    "                       index = documents.index)\n",
    "    \n",
    "    # initialize TfidfTransformer\n",
    "    tfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n",
    "    \n",
    "    # fit/transofrm\n",
    "    X2 = tfidf_engine.fit_transform(DTM)\n",
    "    \n",
    "    # create TFIDF table\n",
    "    TFIDF = pd.DataFrame(X2.toarray(),\n",
    "                         columns = DTM.columns,\n",
    "                         index=DTM.index)\n",
    "    \n",
    "    # create BOW\n",
    "    BOW = DTM[DTM > 0].stack().to_frame('n') \\\n",
    "        .join(TFIDF[TFIDF > 0].stack().to_frame('tfidf'))\n",
    "    BOW.index.rename('term_str', level = 1, inplace = True)\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "755b7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigram BOW\n",
    "BOW_unigrams = createBOW(ARTICLES, ngram_range = (1,1), max_features = 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dd61580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Ngram BOW\n",
    "BOW_ngrams = createBOW(ARTICLES, ngram_range = (1,4), max_features = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c1def",
   "metadata": {},
   "source": [
    "#### Add Frequency Features to `VOCAB`<a name=\"subparagraph3\"></a>\n",
    "I add the following features to the `VOCAB` table.\n",
    "+ `tfidf_mean` - Average tfidf for all occurences in corpus\n",
    "+ `tfidf_max` - Max tfidf value of term\n",
    "+ `df` - Document Frequency, count of documents in which the term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea8f6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CountVectorizer with default params, but custom tokenizer\n",
    "vocab_engine = CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "\n",
    "# fit/transform\n",
    "vocab_counts = vocab_engine.fit_transform(ARTICLES)\n",
    "\n",
    "# convert to dataframe\n",
    "DOC_TERM = pd.DataFrame(vocab_counts.toarray(),\n",
    "                        columns = vocab_engine.get_feature_names_out(),\n",
    "                       index = ARTICLES.index)\n",
    "# append to VOCAB\n",
    "VOCAB['df'] = DOC_TERM[DOC_TERM > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "806f6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TfidfTransformer\n",
    "vocab_tfidf_engine = TfidfTransformer(norm='l2', use_idf=True)\n",
    "\n",
    "# fit/transform\n",
    "vocab_tfidf = vocab_tfidf_engine.fit_transform(DOC_TERM)\n",
    "\n",
    "# create TFIDF dataframe\n",
    "TFIDF = pd.DataFrame(vocab_tfidf.toarray(),\n",
    "                    columns = DOC_TERM.columns,\n",
    "                    index =  DOC_TERM.index)\n",
    "# add to VOCAB\n",
    "VOCAB['tfidf_mean'] = TFIDF.mean()\n",
    "VOCAB['tfidf_max'] = TFIDF.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677ccd5",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving the Digital Analytical Edition<a name=\"paragraph6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4ee14",
   "metadata": {},
   "source": [
    "With the creation of the `LIB`, `CORPUS`, `VOCAB`, and `BOW` tables, I have established a foundation on which I can begin my analysis. I save these tables to separate `.csv` files for ease of use. In the following notebook, I will use each of these tables to perform principal component analysis, topic modelling, word embeddings, and sentiment analysis. Along the way, I will continue to build out the digital analytical edition by adding features to each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03af3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "LIB.to_csv('./data/LIB.csv')\n",
    "CORPUS.to_csv('./data/CORPUS.csv')\n",
    "VOCAB.to_csv('./data/VOCAB.csv')\n",
    "BOW_unigrams.to_csv('./data/BOW_unigrams.csv')\n",
    "BOW_ngrams.to_csv('./data/BOW_ngrams.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
